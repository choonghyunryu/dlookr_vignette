{
  "articles": [
    {
      "path": "diagonosis.html",
      "title": "Data quality diagnosis",
      "description": "Introduce dlookr package for diagnose data quality\n",
      "author": [
        {
          "name": "Choonghyun Ryu",
          "url": "https://dataholic.netlify.app/"
        }
      ],
      "date": "2021-11-28",
      "contents": "\n\nContents\nPreface\nSupported data structures\nData: nycflights13\nData diagnosis\nGeneral diagnosis of all variables with diagnose()\nDiagnosis of numeric variables with diagnose_numeric()\nDiagnosis of categorical variables with diagnose_category()\nDiagnosing outliers with diagnose_outlier()\nVisualization of outliers using plot_outlier()\nVisualization for missing values\n\nAutomated report\nCreate a diagnostic report using diagnose_web_report()\nCreate a diagnostic report using diagnose_paged_report()\n\nDiagnosing tables in DBMS\nPreparing table data\nDiagnose data quality of variables in the DBMS\nDiagnose data quality of categorical variables in the DBMS\nDiagnose data quality of numerical variables in the DBMS\nDiagnose outlier of numerical variables in the DBMS\nPlot outlier information of numerical data diagnosis in the DBMS\nReporting the information of data diagnosis for table of thr DBMS\n\n\n\n\n\nPreface\nAfter you have acquired the data, you should do the following:\nDiagnose data quality.\nIf there is a problem with data quality,\nThe data must be corrected or re-acquired.\n\nExplore data to understand the data and find scenarios for performing the analysis.\nDerive new variables or perform variable transformations.\nThe dlookr package makes these steps fast and easy:\nPerforms a data diagnosis or automatically generates a data diagnosis report.\nDiscover data in a variety of ways, and automatically generate EDA(exploratory data analysis) report.\nImpute missing values and outliers, resolve skewed data, and categorize continuous variables into categorical variables. And generates an automated report to support it.\nThis document introduces Data Quality Diagnosis methods provided by the dlookr package. You will learn how to diagnose the quality of tbl_df data that inherits from data.frame and data.frame with functions provided by dlookr.\ndlookr increases synergy with dplyr. Particularly in data exploration and data wrangle, it increases the efficiency of the tidyverse package group.\nSupported data structures\nData diagnosis supports the following data structures.\ndata frame : data.frame class.\ndata table : tbl_df class.\ntable of DBMS : table of the DBMS through tbl_dbi.\nUse dplyr as the back-end interface for any DBI-compatible database.\n\nData: nycflights13\nTo illustrate basic use of the dlookr package, use the flights data from the nycflights13 package. The flights data frame is data about departure and arrival on all flights departing from NYC in 2013.\n\n\nlibrary(nycflights13)\ndim(flights)\n\n\n[1] 336776     19\n\nflights\n\n\n# A tibble: 336,776 x 19\n   year month   day dep_time sched_dep_time dep_delay arr_time\n  <int> <int> <int>    <int>          <int>     <dbl>    <int>\n1  2013     1     1      517            515         2      830\n2  2013     1     1      533            529         4      850\n3  2013     1     1      542            540         2      923\n4  2013     1     1      544            545        -1     1004\n# … with 336,772 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nData diagnosis\ndlookr aims to diagnose the data and to select variables that can not be used for data analysis or to find the variables that need to be calibrated.:\ndiagnose() provides basic diagnostic information for variables.\ndiagnose_category() provides detailed diagnostic information for categorical variables.\ndiagnose_numeric() provides detailed diagnostic information for numerical variables.\ndiagnose_outlier() and plot_outlier() provide information and visualization of outliers.\nGeneral diagnosis of all variables with diagnose()\ndiagnose() allows to diagnosis a variables in a data frame. Like function of dplyr, the first argument is the tibble (or data frame). The second and subsequent arguments refer to variables within that data frame.\nThe variables of the tbl_df object returned by diagnose () are as follows.\nvariables : variable names\ntypes : the data type of the variables\nmissing_count : number of missing values\nmissing_percent : percentage of missing values\nunique_count : number of unique values\nunique_rate : rate of unique value. unique_count / number of observation\nFor example, we can diagnose all variables in flights:\n\n\ndiagnose(flights)\n\n\n# A tibble: 19 x 6\n  variables types   missing_count missing_percent unique_count\n  <chr>     <chr>           <int>           <dbl>        <int>\n1 year      integer             0            0               1\n2 month     integer             0            0              12\n3 day       integer             0            0              31\n4 dep_time  integer          8255            2.45         1319\n# … with 15 more rows, and 1 more variable: unique_rate <dbl>\n\nMissing Value(NA) : Variables with many missing values, ie those with a missing_percent close to 100, should be excluded from the analysis.\nUnique value : Variables with a unique value (unique_count = 1) are considered to be excluded from data analysis. And if the data type is not numeric (integer, numeric) and the number of unique values is equal to the number of observations (unique_rate = 1), then the variable is likely to be an identifier. Therefore, this variable is also not suitable for the analysis model.\nyear can be considered not to be used in the analysis model since unique_count is 1. However, you do not have to remove it if you configure date as a combination of year, month, and day.\nFor example, we can diagnose only a few selected variables:\n\n\n# Select columns by name\ndiagnose(flights, year, month, day)\n\n\n# A tibble: 3 x 6\n  variables types   missing_count missing_percent unique_count\n  <chr>     <chr>           <int>           <dbl>        <int>\n1 year      integer             0               0            1\n2 month     integer             0               0           12\n3 day       integer             0               0           31\n# … with 1 more variable: unique_rate <dbl>\n\n# Select all columns between year and day (include)\ndiagnose(flights, year:day)\n\n\n# A tibble: 3 x 6\n  variables types   missing_count missing_percent unique_count\n  <chr>     <chr>           <int>           <dbl>        <int>\n1 year      integer             0               0            1\n2 month     integer             0               0           12\n3 day       integer             0               0           31\n# … with 1 more variable: unique_rate <dbl>\n\n# Select all columns except those from year to day (exclude)\ndiagnose(flights, -(year:day))\n\n\n# A tibble: 16 x 6\n  variables      types   missing_count missing_percent unique_count\n  <chr>          <chr>           <int>           <dbl>        <int>\n1 dep_time       integer          8255            2.45         1319\n2 sched_dep_time integer             0            0            1021\n3 dep_delay      numeric          8255            2.45          528\n4 arr_time       integer          8713            2.59         1412\n# … with 12 more rows, and 1 more variable: unique_rate <dbl>\n\nBy using with dplyr, variables including missing values can be sorted by the weight of missing values.:\n\n\nflights %>%\n  diagnose() %>%\n  select(-unique_count, -unique_rate) %>% \n  filter(missing_count > 0) %>% \n  arrange(desc(missing_count))\n\n\n# A tibble: 6 x 4\n  variables types   missing_count missing_percent\n  <chr>     <chr>           <int>           <dbl>\n1 arr_delay numeric          9430            2.80\n2 air_time  numeric          9430            2.80\n3 arr_time  integer          8713            2.59\n4 dep_time  integer          8255            2.45\n# … with 2 more rows\n\nDiagnosis of numeric variables with diagnose_numeric()\ndiagnose_numeric() diagnoses numeric(continuous and discrete) variables in a data frame. Usage is the same as diagnose() but returns more diagnostic information. However, if you specify a non-numeric variable in the second and subsequent argument list, the variable is automatically ignored.\nThe variables of the tbl_df object returned by diagnose_numeric() are as follows.\nmin : minimum value\nQ1 : 1/4 quartile, 25th percentile\nmean : arithmetic mean\nmedian : median, 50th percentile\nQ3 : 3/4 quartile, 75th percentile\nmax : maximum value\nzero : number of observations with a value of 0\nminus : number of observations with negative numbers\noutlier : number of outliers\nThe summary() function summarizes the distribution of individual variables in the data frame and outputs it to the console. The summary values of numeric variables are min, Q1, mean, median, Q3 and max, which help to understand the distribution of data.\nHowever, the result displayed on the console has the disadvantage that the analyst has to look at it with the eyes. However, when the summary information is returned in a data frame structure such as tbl_df, the scope of utilization is expanded. diagnose_numeric() supports this.\nzero, minus, and outlier are useful measures to diagnose data integrity. For example, numerical data in some cases cannot have zero or negative numbers. A numeric variable called employee salary cannot have negative numbers or zeros. Therefore, this variable should be checked for the inclusion of zero or negative numbers in the data diagnosis process.\ndiagnose_numeric() can diagnose all numeric variables of flights as follows.:\n\n\ndiagnose_numeric(flights)\n\n\n# A tibble: 14 x 10\n  variables   min    Q1    mean median    Q3   max  zero minus outlier\n  <chr>     <dbl> <dbl>   <dbl>  <dbl> <dbl> <dbl> <int> <int>   <int>\n1 year       2013  2013 2013      2013  2013  2013     0     0       0\n2 month         1     4    6.55      7    10    12     0     0       0\n3 day           1     8   15.7      16    23    31     0     0       0\n4 dep_time      1   907 1349.     1401  1744  2400     0     0       0\n# … with 10 more rows\n\nIf a numeric variable can not logically have a negative or zero value, it can be used with filter() to easily find a variable that does not logically match:\n\n\ndiagnose_numeric(flights) %>% \n  filter(minus > 0 | zero > 0) \n\n\n# A tibble: 3 x 10\n  variables   min    Q1  mean median    Q3   max  zero  minus outlier\n  <chr>     <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <int>  <int>   <int>\n1 dep_delay   -43    -5 12.6      -2    11  1301 16514 183575   43216\n2 arr_delay   -86   -17  6.90     -5    14  1272  5409 188933   27880\n3 minute        0     8 26.2      29    44    59 60696      0       0\n\nDiagnosis of categorical variables with diagnose_category()\ndiagnose_category() diagnoses the categorical(factor, ordered, character) variables of a data frame. The usage is similar to diagnose() but returns more diagnostic information. If you specify a non-categorical variable in the second and subsequent argument list, the variable is automatically ignored.\nThe top argument specifies the number of levels to return for each variable. The default is 10, which returns the top 10 level. Of course, if the number of levels is less than 10, all levels are returned.\nThe variables of the tbl_df object returned by diagnose_category() are as follows.\nvariables : variable names\nlevels: level names\nN : number of observation\nfreq : number of observation at the levels\nratio : percentage of observation at the levels\nrank : rank of occupancy ratio of levels\n`diagnose_category() can diagnose all categorical variables of flights as follows.:\n\n\ndiagnose_category(flights)\n\n\n# A tibble: 43 x 6\n  variables levels      N  freq ratio  rank\n  <chr>     <chr>   <int> <int> <dbl> <int>\n1 carrier   UA     336776 58665  17.4     1\n2 carrier   B6     336776 54635  16.2     2\n3 carrier   EV     336776 54173  16.1     3\n4 carrier   DL     336776 48110  14.3     4\n# … with 39 more rows\n\nIn collaboration with filter() in the dplyr package, we can see that the tailnum variable is ranked in top 1 with 2,512 missing values in the case where the missing value is included in the top 10:\n\n\ndiagnose_category(flights) %>% \n  filter(is.na(levels))\n\n\n# A tibble: 1 x 6\n  variables levels      N  freq ratio  rank\n  <chr>     <chr>   <int> <int> <dbl> <int>\n1 tailnum   <NA>   336776  2512 0.746     1\n\nThe following example returns a list where the level’s relative percentage is 0.01% or less. Note that the value of the top argument is set to a large value such as 500. If the default value of 10 was used, values below 0.01% would not be included in the list:\n\n\nflights %>%\n  diagnose_category(top = 500)  %>%\n  filter(ratio <= 0.01)\n\n\n# A tibble: 10 x 6\n  variables levels      N  freq   ratio  rank\n  <chr>     <chr>   <int> <int>   <dbl> <int>\n1 carrier   OO     336776    32 0.00950    16\n2 dest      JAC    336776    25 0.00742    97\n3 dest      PSP    336776    19 0.00564    98\n4 dest      EYW    336776    17 0.00505    99\n# … with 6 more rows\n\nIn the analytics model, you can also consider removing levels where the relative frequency is very small in the observations or, if possible, combining them together.\nDiagnosing outliers with diagnose_outlier()\ndiagnose_outlier() diagnoses the outliers of the numeric (continuous and discrete) variables of the data frame. The usage is the same as diagnose().\nThe variables of the tbl_df object returned by diagnose_outlier() are as follows.\noutliers_cnt : number of outliers\noutliers_ratio : percent of outliers\noutliers_mean : arithmetic average of outliers\nwith_mean : arithmetic average of with outliers\nwithout_mean : arithmetic average of without outliers\ndiagnose_outlier() can diagnose outliers of all numerical variables on flights as follows:\n\n\ndiagnose_outlier(flights)\n\n\n# A tibble: 14 x 6\n  variables outliers_cnt outliers_ratio outliers_mean with_mean\n  <chr>            <int>          <dbl>         <dbl>     <dbl>\n1 year                 0              0           NaN   2013   \n2 month                0              0           NaN      6.55\n3 day                  0              0           NaN     15.7 \n4 dep_time             0              0           NaN   1349.  \n# … with 10 more rows, and 1 more variable: without_mean <dbl>\n\nNumeric variables that contained outliers are easily found with filter().:\n\n\ndiagnose_outlier(flights) %>% \n  filter(outliers_cnt > 0) \n\n\n# A tibble: 5 x 6\n  variables outliers_cnt outliers_ratio outliers_mean with_mean\n  <chr>            <int>          <dbl>         <dbl>     <dbl>\n1 dep_delay        43216      12.8               93.1     12.6 \n2 arr_delay        27880       8.28             121.       6.90\n3 flight               1       0.000297        8500     1972.  \n4 air_time          5448       1.62             400.     151.  \n# … with 1 more row, and 1 more variable: without_mean <dbl>\n\nThe following example finds a numeric variable with an outlier ratio of 5% or more, and then returns the result of dividing mean of outliers by overall mean in descending order:\n\n\ndiagnose_outlier(flights) %>% \n  filter(outliers_ratio > 5) %>% \n  mutate(rate = outliers_mean / with_mean) %>% \n  arrange(desc(rate)) %>% \n  select(-outliers_cnt)\n\n\n# A tibble: 2 x 6\n  variables outliers_ratio outliers_mean with_mean without_mean  rate\n  <chr>              <dbl>         <dbl>     <dbl>        <dbl> <dbl>\n1 arr_delay           8.28         121.       6.90       -3.69  17.5 \n2 dep_delay          12.8           93.1     12.6         0.444  7.37\n\nIn cases where the mean of the outliers is large relative to the overall average, it may be desirable to impute or remove the outliers.\nVisualization of outliers using plot_outlier()\nplot_outlier() visualizes outliers of numerical variables(continuous and discrete) of data.frame. Usage is the same diagnose().\nThe plot derived from the numerical data diagnosis is as follows.\nWith outliers box plot\nWithout outliers box plot\nWith outliers histogram\nWithout outliers histogram\nplot_outlier() can visualize an outliers in the arr_delay variable of flights as follows:\n\n\nflights %>%\n  plot_outlier(arr_delay) \n\n\n\n\nThe following example uses diagnose_outlier(), plot_outlier(), and dplyr packages to visualize all numerical variables with an outlier ratio of 5% or higher.\n\n\nflights %>%\n  plot_outlier(diagnose_outlier(flights) %>% \n                 filter(outliers_ratio >= 5) %>% \n                 select(variables) %>% \n                 unlist())\n\n\n\nAnalysts should look at the results of the visualization to decide whether to remove or replace outliers. In some cases, you should consider removing variables with outliers from the data analysis model.\nLooking at the results of the visualization, arr_delay shows that the observed values without outliers are similar to the normal distribution. In the case of a linear model, we might consider removing or imputing outliers.\nVisualization for missing values\nIt is important to look at the missing values of individual variables, but it is also important to look at the relationship between the variables including the missing values.\ndlookr provides a visualization tool that looks at the relationship of variables including missing values.\nvisualize pareto chart using plot_na_pareto()\nplot_na_pareto() draws a pareto chart by collecting variables including missing values.\n\n\nmice::boys %>% \n  plot_na_pareto(col = \"blue\")\n\n\n\n\nThe default value of the only_na argument is FALSE, which includes variables that do not contain missing values, but if this value is set to TRUE, only variables containing missing values are visualized. The variable age was excluded from this plot.\n\n\nmice::boys %>% \n  plot_na_pareto(only_na = TRUE, main = \"Pareto Chart for mice::boys\")\n\n\n\nThe rating of the variable is expressed as a proportion of missing values. It is calculated as the ratio of missing values. If it is [0, 0.05), it is Good, if it is [0.05, 0.4) it is OK, if it is [0.4, 0.8) it is Bad, and if it is [0.8, 1.0] it is Remove. You can override this grade using the grade argument as follows:\n\n\nmice::boys %>% \n  plot_na_pareto(grade = list(High = 0.1, Middle = 0.6, Low = 1), relative = TRUE)\n\n\n\nIf the plot argument is set to FALSE, information about missing values is returned instead of plotting.\n\n\nplot_na_pareto(mice::boys, only_na = TRUE, plot = FALSE)\n\n\n\nvisualize combination chart using plot_na_hclust()\nIt is important to look at the relationship between variables, including missing values. plot_na_hclust() visualizes the relationship of variables that contain missing values. This function rearranges the positions of variables using hierarchical clustering. Then, the expression of the missing value is visualized by grouping similar variables.\n\n\nmice::boys %>% \n  plot_na_hclust(main = \"Distribution of missing value\")\n\n\n\n\nvisualize combination chart using plot_na_intersect()\nplot_na_intersect() visualize the combinations of missing value across cases.\nThe visualization consists of four parts. The bottom left, which is the most basic, visualizes the case of cross(intersection)-combination. The x-axis is the variable including the missing value, and the y-axis represents the case of a combination of variables. And on the marginal of the two axes, the frequency of the case is expressed as a bar graph. Finally, the visualization at the top right expresses the number of variables including missing values in the data set, and the number of observations including missing values and complete cases .\nThis example visualize the combination variables that is include missing value.\n\n\nmice::boys %>% \n  plot_na_intersect()\n\n\n\n\nIf the n_vars argument is used, only the top n variables containing many missing values are visualized.\n\n\nmice::boys %>%\n  plot_na_intersect(n_vars = 5)\n\n\n\nIf you use the n_intersacts argument, only the top n numbers of variable combinations(intersection) including missing values are visualized. and If you want to visualize the combination variables that is include missing value and complete case. You just add only_na = FALSE.\n\n\nmice::boys %>%\n  plot_na_intersect(only_na = FALSE, n_intersacts = 7)\n\n\n\nAutomated report\ndlookr provides two automated data diagnostic reports:\nWeb page-based dynamic reports can perform in-depth analysis through visualization and statistical tables.\nStatic reports generated as pdf files or html files can be archived as output of data analysis.\nCreate a diagnostic report using diagnose_web_report()\ndiagnose_web_report() create dynamic report for object inherited from data.frame(tbl_df, tbl, etc) or data.frame.\nContents of dynamic web report\nThe contents of the report are as follows.:\nOverview\nData Structures\nData Structures\nData Types\nJob Informations\n\nWarnings\nVariables\n\nMissing Values\nList of Missing Values\nVisualization\n\nUnique Values\nCategorical Variables\nNumerical Variables\n\nOutliers\nSamples\nDuplicated\nHeads\nTails\n\nSome arguments for dynamic web report\ndiagnose_web_report() generates various reports with the following arguments.\noutput_file\nname of generated file.\n\noutput_dir\nname of directory to generate report file.\n\ntitle\ntitle of report.\n\nsubtitle\nsubtitle of report.\n\nauthor\nauthor of report.\n\ntitle_color\ncolor of title.\n\nthres_uniq_cat\nthreshold to use for “Unique Values - Categorical Variables”.\n\nthres_uniq_num\nthreshold to use for “Unique Values - Numerical Variables”.\n\nlogo_img\nname of logo image file on top left.\n\ncreate_date\nThe date on which the report is generated.\n\ntheme\nname of theme for report. support “orange” and “blue”.\n\nsample_percent\nSample percent of data for performing Diagnosis.\n\nThe following script creates a quality diagnosis report for the tbl_df class object, flights.\n\n\nflights %>%\n  diagnose_web_report(subtitle = \"flights\", output_dir = \"./\", \n                      output_file = \"Diagn.html\", theme = \"blue\")\n\n\n\nScreenshot of dynamic report\nThe part of the report is shown in the following figure.:\n\n\n\n(#fig:diag_web_title)The part of the report\n\n\n\nThe dynamic contents of the report is shown in the following figure.:\n\n\n\n(#fig:diag_web_content)The dynamic contents of the report\n\n\n\nCreate a diagnostic report using diagnose_paged_report()\ndiagnose_paged_report() create static report for object inherited from data.frame(tbl_df, tbl, etc) or data.frame.\nContents of static paged report\nThe contents of the report are as follows.:\nOverview\nData Structures\nJob Informations\nWarnings\nVariables\n\nMissing Values\nList of Missing Values\nVisualization\n\nUnique Values\nCategorical Variables\nNumerical Variables\n\nCategorical Variable Diagnosis\nTop Ranks\n\nNumerical Variable Diagnosis\nDistribution\nZero Values\nMinus Values\n\nOutliers\nList of Outliers\nIndividual Outliers\n\n\nSome arguments for static paged report\ndiagnose_paged_report() generates various reports with the following arguments.\noutput_format\nreport output type. Choose either “pdf” and “html”.\n\noutput_file\nname of generated file.\n\noutput_dir\nname of directory to generate report file.\n\ntitle\ntitle of report.\n\nsubtitle\nsubtitle of report.\n\nabstract_title\nabstract of report\n\nauthor\nauthor of report.\n\ntitle_color\ncolor of title.\n\nsubtitle_color\ncolor of subtitle.\n\nthres_uniq_cat\nthreshold to use for “Unique Values - Categorical Variables”.\n\nthres_uniq_num\nthreshold to use for “Unique Values - Numerical Variables”.\n\nflag_content_zero\nwhether to output “Zero Values” information.\n\nflag_content_minus\nwhether to output “Minus Values” information.\n\nflag_content_missing\nwhether to output “Missing Value” information.\n\nlogo_img\nname of logo image file on top left.\n\ncover_img\nname of cover image file on center.\n\ncreate_date\nThe date on which the report is generated.\n\ntheme\nname of theme for report. support “orange” and “blue”.\n\nsample_percent\nSample percent of data for performing Diagnosis.\n\nThe following script creates a quality diagnosis report for the tbl_df class object, flights.\n\n\nflights %>%\n  diagnose_paged_report(subtitle = \"flights\", output_dir = \"./\",\n                        output_file = \"Diagn.pdf\", theme = \"blue\")\n\n\n\nScreenshot of static report\nThe cover of the report is shown in the following figure.:\n\n\n\n(#fig:diag_paged_cover)The part of the report\n\n\n\nThe contents of the report is shown in the following figure.:\n\n\n\n(#fig:diag_paged_cntent)The dynamic contents of the report\n\n\n\nDiagnosing tables in DBMS\nThe DBMS table diagnostic function supports In-database mode that performs SQL operations on the DBMS side. If the size of the data is large, using In-database mode is faster.\nIt is difficult to obtain anomaly or to implement the sampling-based algorithm in SQL of DBMS. So some functions do not yet support In-database mode. In this case, it is performed in In-memory mode in which table data is brought to R side and calculated. In this case, if the data size is large, the execution speed may be slow. It supports the collect_size argument, which allows you to import the specified number of samples of data into R.\nIn-database support functions\ndiagonse()\ndiagnose_category()\n\nIn-database not support functions\ndiagnose_numeric()\ndiagnose_outlier()\nplot_outlier()\ndiagnose_web_report()\ndiagnose_paged_report()\n\nPreparing table data\nCopy the carseats data frame to the SQLite DBMS and create it as a table named TB_CARSEATS. Mysql/MariaDB, PostgreSQL, Oracle DBMS, other DBMS are also available for your environment.\n\n\nif (!require(DBI)) install.packages('DBI')\nif (!require(RSQLite)) install.packages('RSQLite')\nif (!require(dplyr)) install.packages('dplyr')\nif (!require(dbplyr)) install.packages('dbplyr')\n\nlibrary(dplyr)\n\ncarseats <- ISLR::Carseats\ncarseats[sample(seq(NROW(carseats)), 20), \"Income\"] <- NA\ncarseats[sample(seq(NROW(carseats)), 5), \"Urban\"] <- NA\n\n# connect DBMS\ncon_sqlite <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\n\n# copy carseats to the DBMS with a table named TB_CARSEATS\ncopy_to(con_sqlite, carseats, name = \"TB_CARSEATS\", overwrite = TRUE)\n\n\n\nDiagnose data quality of variables in the DBMS\nUse dplyr::tbl() to create a tbl_dbi object, then use it as a data frame object. That is, the data argument of all diagnose function is specified as tbl_dbi object instead of data frame object.\n\n\n# Diagnosis of all columns\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose()\n\n\n# A tibble: 11 x 6\n  variables   types  missing_count missing_percent unique_count\n  <chr>       <chr>          <dbl>           <dbl>        <int>\n1 Sales       double             0               0          336\n2 CompPrice   double             0               0           73\n3 Income      double            20               5           97\n4 Advertising double             0               0           28\n# … with 7 more rows, and 1 more variable: unique_rate <dbl>\n\n\n# Positions values select columns, and In-memory mode\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose(1, 3, 8, in_database = FALSE)\n\n\n# A tibble: 3 x 6\n  variables types   missing_count missing_percent unique_count\n  <chr>     <chr>           <int>           <dbl>        <int>\n1 Sales     numeric             0               0          336\n2 Income    numeric            20               5           97\n3 Age       numeric             0               0           56\n# … with 1 more variable: unique_rate <dbl>\n\n  \n# Positions values select columns, and In-memory mode and collect size is 200\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose(-8, -9, -10, in_database = FALSE, collect_size = 200)\n\n\n# A tibble: 8 x 6\n  variables   types   missing_count missing_percent unique_count\n  <chr>       <chr>           <int>           <dbl>        <int>\n1 Sales       numeric             0               0          182\n2 CompPrice   numeric             0               0           65\n3 Income      numeric             6               3           83\n4 Advertising numeric             0               0           23\n# … with 4 more rows, and 1 more variable: unique_rate <dbl>\n\nDiagnose data quality of categorical variables in the DBMS\n\n\n# Positions values select variables, and In-memory mode and collect size is 200\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose_category(7, in_database = FALSE, collect_size = 200) \n\n\n# A tibble: 3 x 6\n  variables levels     N  freq ratio  rank\n  <chr>     <chr>  <int> <int> <dbl> <int>\n1 ShelveLoc Medium   200   113  56.5     1\n2 ShelveLoc Bad      200    47  23.5     2\n3 ShelveLoc Good     200    40  20       3\n\n  \n# Positions values select variables\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose_category(-7)\n\n\n# A tibble: 5 x 6\n  variables levels     N  freq ratio  rank\n  <chr>     <chr>  <int> <int> <dbl> <int>\n1 Urban     Yes      400   277 69.2      1\n2 Urban     No       400   118 29.5      2\n3 Urban     <NA>     400     5  1.25     3\n4 US        Yes      400   258 64.5      1\n# … with 1 more row\n\nDiagnose data quality of numerical variables in the DBMS\n\n\n# Diagnosis of all numerical variables\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose_numeric()\n\n\n# A tibble: 8 x 10\n  variables     min     Q1   mean median     Q3   max  zero minus\n  <chr>       <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <dbl> <int> <int>\n1 Sales           0   5.39   7.50   7.49   9.32  16.3     1     0\n2 CompPrice      77 115    125.   125    135    175       0     0\n3 Income         21  42     68.2   69     90.2  120       0     0\n4 Advertising     0   0      6.64   5     12     29     144     0\n# … with 4 more rows, and 1 more variable: outlier <int>\n\n  \n# Positive values select variables, and In-memory mode and collect size is 200\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose_numeric(Sales, Income, collect_size = 200)\n\n\n# A tibble: 2 x 10\n  variables   min    Q1  mean median    Q3   max  zero minus outlier\n* <chr>     <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <int> <int>   <int>\n1 Sales         0  5.26  7.42   7.50  9.10  14.9     1     0       0\n2 Income       21 48    71.4   73    93    120       0     0       0\n\nDiagnose outlier of numerical variables in the DBMS\n\n\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose_outlier()  %>%\n  filter(outliers_ratio > 1)\n\n\n# A tibble: 1 x 6\n  variables outliers_cnt outliers_ratio outliers_mean with_mean\n  <chr>            <int>          <dbl>         <dbl>     <dbl>\n1 Price                5           1.25          100.      116.\n# … with 1 more variable: without_mean <dbl>\n\nPlot outlier information of numerical data diagnosis in the DBMS\n\n\n# Visualization of numerical variables with a ratio of\n# outliers greater than 1%\n# the result is same as a data.frame, but not display here. reference above in document.\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  plot_outlier(con_sqlite %>% \n                 tbl(\"TB_CARSEATS\") %>% \n                 diagnose_outlier() %>%\n                 filter(outliers_ratio > 1) %>%\n                 select(variables) %>%\n                 pull())\n\n\n\nReporting the information of data diagnosis for table of thr DBMS\nThe following shows several examples of creating an data diagnosis report for a DBMS table.\nUsing the collect_size argument, you can perform data diagnosis with the corresponding number of sample data. If the number of data is very large, use collect_size.\n\n\n# create web report file. \ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose_web_report()\n  \n# create pdf file. file name is Diagn.pdf, and collect size is 350\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose_paged_report(collect_size = 350, output_file = \"Diagn.pdf\")\n\n\n\n\n\n\n",
      "last_modified": "2021-11-28T14:59:19+09:00"
    },
    {
      "path": "EDA.html",
      "title": "Exploratory Data Analysis",
      "description": "Introduce dlookr package for explore data to understand the data and find scenarios for performing the analysis.\n",
      "author": [
        {
          "name": "Choonghyun Ryu",
          "url": "https://dataholic.netlify.app/"
        }
      ],
      "date": "2021-11-28",
      "contents": "\n\nContents\nPreface\nSupported data structures\ndatasets\nExploratory Data Analysis\nUnivariate data EDA\nCalculating descriptive statistics using describe()\nTest of normality on numeric variables using normality()\nVisualization of normality of numerical variables using plot_normality()\n\nEDA of bivariate data\nCalculation of correlation coefficient using correlate()\nVisualization of the correlation matrix using plot_correlate()\n\nEDA based on target variable\nDefinition of target variable\nEDA when target variable is categorical variable\nEDA when target variable is numerical variable\n\nAutomated report\nCreate a dynamic report using eda_web_report()\nCreate a EDA report using eda_paged_report()\n\nExploratory data analysis for tables in DBMS\nPreparing table data\nCalculating descriptive statistics of numerical column of table in the DBMS\nTest of normality on numeric columns using in the DBMS\nNormalization visualization of numerical column in the DBMS\nCompute the correlation coefficient between two columns of table in DBMS\nVisualize correlation plot of numerical columns in the DBMS\nEDA based on target variable\nReporting the information of EDA for table of the DBMS\n\n\n\n\n\nPreface\nAfter you have acquired the data, you should do the following:\nDiagnose data quality.\nIf there is a problem with data quality,\nThe data must be corrected or re-acquired.\n\nExplore data to understand the data and find scenarios for performing the analysis.\nDerive new variables or perform variable transformations.\nThe dlookr package makes these steps fast and easy:\nPerforms an data diagnosis or automatically generates a data diagnosis report.\nDiscover data in a variety of ways, and automatically generate EDA(exploratory data analysis) report.\nImpute missing values and outliers, resolve skewed data, and categorize continuous variables into categorical variables. And generates an automated report to support it.\nThis document introduces EDA(Exploratory Data Analysis) methods provided by the dlookr package. You will learn how to EDA of tbl_df data that inherits from data.frame and data.frame with functions provided by dlookr.\ndlookr increases synergy with dplyr. Particularly in data exploration and data wrangle, it increases the efficiency of the tidyverse package group.\nSupported data structures\nData diagnosis supports the following data structures.\ndata frame : data.frame class.\ndata table : tbl_df class.\ntable of DBMS : table of the DBMS through tbl_dbi.\nUse dplyr as the back-end interface for any DBI-compatible database.\n\ndatasets\nTo illustrate the basic use of EDA in the dlookr package, I use a Carseats dataset. Carseats in the ISLR package is a simulated data set containing sales of child car seats at 400 different stores. This data is a data.frame created for the purpose of predicting sales volume.\n\n\nlibrary(ISLR)\nstr(Carseats)\n\n\n'data.frame':   400 obs. of  11 variables:\n $ Sales      : num  9.5 11.22 10.06 7.4 4.15 ...\n $ CompPrice  : num  138 111 113 117 141 124 115 136 132 132 ...\n $ Income     : num  73 48 35 100 64 113 105 81 110 113 ...\n $ Advertising: num  11 16 10 4 3 13 0 15 0 0 ...\n $ Population : num  276 260 269 466 340 501 45 425 108 131 ...\n $ Price      : num  120 83 80 97 128 72 108 120 124 124 ...\n $ ShelveLoc  : Factor w/ 3 levels \"Bad\",\"Good\",\"Medium\": 1 2 3 3 1 1 3 2 3 3 ...\n $ Age        : num  42 65 59 55 38 78 71 67 76 76 ...\n $ Education  : num  17 10 12 14 13 16 15 10 10 17 ...\n $ Urban      : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 1 2 2 1 1 ...\n $ US         : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 1 2 1 2 ...\n\nThe contents of individual variables are as follows. (Refer to ISLR::Carseats Man page)\nSales\nUnit sales (in thousands) at each location\n\nCompPrice\nPrice charged by competitor at each location\n\nIncome\nCommunity income level (in thousands of dollars)\n\nAdvertising\nLocal advertising budget for company at each location (in thousands of dollars)\n\nPopulation\nPopulation size in region (in thousands)\n\nPrice\nPrice company charges for car seats at each site\n\nShelveLoc\nA factor with levels Bad, Good and Medium indicating the quality of the shelving location for the car seats at each site\n\nAge\nAverage age of the local population\n\nEducation\nEducation level at each location\n\nUrban\nA factor with levels No and Yes to indicate whether the store is in an urban or rural location\n\nUS\nA factor with levels No and Yes to indicate whether the store is in the US or not\n\nWhen data analysis is performed, data containing missing values is frequently encountered. However, ‘Carseats’ is complete data without missing values. So the following script created the missing values and saved them as carseats.\n\n\ncarseats <- ISLR::Carseats\n\nsuppressWarnings(RNGversion(\"3.5.0\"))\nset.seed(123)\ncarseats[sample(seq(NROW(carseats)), 20), \"Income\"] <- NA\n\nsuppressWarnings(RNGversion(\"3.5.0\"))\nset.seed(456)\ncarseats[sample(seq(NROW(carseats)), 10), \"Urban\"] <- NA\n\n\n\nExploratory Data Analysis\ndlookr can help to understand the distribution of data by calculating descriptive statistics of numerical data. In addition, correlation between variables is identified and normality test is performed. It also identifies the relationship between target variables and independent variables.:\nThe following is a list of the EDA functions included in the dlookr package.:\ndescribe() provides descriptive statistics for numerical data.\nnormality() and plot_normality() perform normalization and visualization of numerical data.\ncorrelate() and plot_correlate() calculate the correlation coefficient between two numerical data and provide visualization.\ntarget_by() defines the target variable and relate() describes the relationship with the variables of interest corresponding to the target variable.\nplot.relate() visualizes the relationship to the variable of interest corresponding to the destination variable.\neda_report() performs an exploratory data analysis and reports the results.\nUnivariate data EDA\nCalculating descriptive statistics using describe()\ndescribe() computes descriptive statistics for numerical data. The descriptive statistics help determine the distribution of numerical variables. Like function of dplyr, the first argument is the tibble (or data frame). The second and subsequent arguments refer to variables within that data frame.\nThe variables of the tbl_df object returned by describe() are as follows.\nn : number of observations excluding missing values\nna : number of missing values\nmean : arithmetic average\nsd : standard deviation\nse_mean : standard error mean. sd/sqrt(n)\nIQR : interquartile range (Q3-Q1)\nskewness : skewness\nkurtosis : kurtosis\np25 : Q1. 25% percentile\np50 : median. 50% percentile\np75 : Q3. 75% percentile\np01, p05, p10, p20, p30 : 1%, 5%, 20%, 30% percentiles\np40, p60, p70, p80 : 40%, 60%, 70%, 80% percentiles\np90, p95, p99, p100 : 90%, 95%, 99%, 100% percentiles\nFor example, describe() can computes the statistics of all numerical variables in carseats:\n\n\ndescribe(carseats)\n\n\n# A tibble: 8 x 26\n  variable        n    na   mean    sd se_mean   IQR skewness kurtosis\n  <chr>       <int> <int>  <dbl> <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n1 Sales         400     0   7.50  2.82   0.141  3.93   0.186   -0.0809\n2 CompPrice     400     0 125.   15.3    0.767 20     -0.0428   0.0417\n3 Income        380    20  68.9  28.1    1.44  48.2    0.0449  -1.09  \n4 Advertising   400     0   6.64  6.65   0.333 12      0.640   -0.545 \n# … with 4 more rows, and 17 more variables: p00 <dbl>, p01 <dbl>,\n#   p05 <dbl>, p10 <dbl>, p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>,\n#   p50 <dbl>, p60 <dbl>, p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>,\n#   p95 <dbl>, p99 <dbl>, p100 <dbl>\n\nskewness : The left-skewed distribution data that is the variables with large positive skewness should consider the log or sqrt transformations to follow the normal distribution. The variables Advertising seem to need to consider variable transformation.\nmean and sd, se_mean : ThePopulation with a large standard error of the mean(se_mean) has low representativeness of the arithmetic mean(mean). The standard deviation(sd) is much larger than the arithmetic average.\nThe following explains the descriptive statistics only for a few selected variables.:\n\n\n# Select columns by name\ndescribe(carseats, Sales, CompPrice, Income)\n\n\n# A tibble: 3 x 26\n  variable      n    na   mean    sd se_mean   IQR skewness kurtosis\n  <chr>     <int> <int>  <dbl> <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n1 Sales       400     0   7.50  2.82   0.141  3.93   0.186   -0.0809\n2 CompPrice   400     0 125.   15.3    0.767 20     -0.0428   0.0417\n3 Income      380    20  68.9  28.1    1.44  48.2    0.0449  -1.09  \n# … with 17 more variables: p00 <dbl>, p01 <dbl>, p05 <dbl>,\n#   p10 <dbl>, p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>, p50 <dbl>,\n#   p60 <dbl>, p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>, p95 <dbl>,\n#   p99 <dbl>, p100 <dbl>\n\n# Select all columns between year and day (include)\ndescribe(carseats, Sales:Income)\n\n\n# A tibble: 3 x 26\n  variable      n    na   mean    sd se_mean   IQR skewness kurtosis\n  <chr>     <int> <int>  <dbl> <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n1 Sales       400     0   7.50  2.82   0.141  3.93   0.186   -0.0809\n2 CompPrice   400     0 125.   15.3    0.767 20     -0.0428   0.0417\n3 Income      380    20  68.9  28.1    1.44  48.2    0.0449  -1.09  \n# … with 17 more variables: p00 <dbl>, p01 <dbl>, p05 <dbl>,\n#   p10 <dbl>, p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>, p50 <dbl>,\n#   p60 <dbl>, p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>, p95 <dbl>,\n#   p99 <dbl>, p100 <dbl>\n\n# Select all columns except those from year to day (exclude)\ndescribe(carseats, -(Sales:Income))\n\n\n# A tibble: 5 x 26\n  variable       n    na   mean     sd se_mean   IQR skewness kurtosis\n  <chr>      <int> <int>  <dbl>  <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n1 Advertisi…   400     0   6.64   6.65   0.333  12     0.640    -0.545\n2 Population   400     0 265.   147.     7.37  260.   -0.0512   -1.20 \n3 Price        400     0 116.    23.7    1.18   31    -0.125     0.452\n4 Age          400     0  53.3   16.2    0.810  26.2  -0.0772   -1.13 \n# … with 1 more row, and 17 more variables: p00 <dbl>, p01 <dbl>,\n#   p05 <dbl>, p10 <dbl>, p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>,\n#   p50 <dbl>, p60 <dbl>, p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>,\n#   p95 <dbl>, p99 <dbl>, p100 <dbl>\n\nThe describe() function can be sorted by left or right skewed size(skewness) using dplyr.:\n\n\ncarseats %>%\n  describe() %>%\n  select(variable, skewness, mean, p25, p50, p75) %>% \n  filter(!is.na(skewness)) %>% \n  arrange(desc(abs(skewness)))\n\n\n# A tibble: 8 x 6\n  variable    skewness   mean    p25    p50    p75\n  <chr>          <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1 Advertising   0.640    6.64   0      5     12   \n2 Sales         0.186    7.50   5.39   7.49   9.32\n3 Price        -0.125  116.   100    117    131   \n4 Age          -0.0772  53.3   39.8   54.5   66   \n# … with 4 more rows\n\nThe describe() function supports the group_by() function syntax of the dplyr package.\n\n\ncarseats %>%\n  group_by(US) %>% \n  describe(Sales, Income) \n\n\n# A tibble: 4 x 27\n  variable US        n    na  mean    sd se_mean   IQR skewness\n  <chr>    <fct> <int> <int> <dbl> <dbl>   <dbl> <dbl>    <dbl>\n1 Income   No      130    12 65.8  28.2    2.48  50      0.100 \n2 Income   Yes     250     8 70.4  27.9    1.77  48      0.0199\n3 Sales    No      142     0  6.82  2.60   0.218  3.44   0.323 \n4 Sales    Yes     258     0  7.87  2.88   0.179  4.23   0.0760\n# … with 18 more variables: kurtosis <dbl>, p00 <dbl>, p01 <dbl>,\n#   p05 <dbl>, p10 <dbl>, p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>,\n#   p50 <dbl>, p60 <dbl>, p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>,\n#   p95 <dbl>, p99 <dbl>, p100 <dbl>\n\n\n\ncarseats %>%\n  group_by(US, Urban) %>% \n  describe(Sales, Income) \n\n\n# A tibble: 12 x 28\n  variable US    Urban     n    na  mean    sd se_mean   IQR skewness\n  <chr>    <fct> <fct> <int> <int> <dbl> <dbl>   <dbl> <dbl>    <dbl>\n1 Income   No    No       42     4  60.2  29.1    4.49  45.2   0.408 \n2 Income   No    Yes      84     8  69.5  27.4    2.99  47    -0.0497\n3 Income   No    <NA>      4     0  48.2  24.7   12.3   40.8  -0.0496\n4 Income   Yes   No       65     4  70.5  29.9    3.70  48     0.0736\n# … with 8 more rows, and 18 more variables: kurtosis <dbl>,\n#   p00 <dbl>, p01 <dbl>, p05 <dbl>, p10 <dbl>, p20 <dbl>, p25 <dbl>,\n#   p30 <dbl>, p40 <dbl>, p50 <dbl>, p60 <dbl>, p70 <dbl>, p75 <dbl>,\n#   p80 <dbl>, p90 <dbl>, p95 <dbl>, p99 <dbl>, p100 <dbl>\n\nTest of normality on numeric variables using normality()\nnormality() performs a normality test on numerical data. Shapiro-Wilk normality test is performed. When the number of observations is greater than 5000, it is tested after extracting 5000 samples by random simple sampling.\nThe variables of tbl_df object returned by normality() are as follows.\nstatistic : Statistics of the Shapiro-Wilk test\np_value : p-value of the Shapiro-Wilk test\nsample : Number of sample observations performed Shapiro-Wilk test\nnormality() performs the normality test for all numerical variables of carseats as follows.:\n\n\nnormality(carseats)\n\n\n# A tibble: 8 x 4\n  vars        statistic  p_value sample\n  <chr>           <dbl>    <dbl>  <dbl>\n1 Sales           0.995 2.54e- 1    400\n2 CompPrice       0.998 9.77e- 1    400\n3 Income          0.961 1.52e- 8    400\n4 Advertising     0.874 1.49e-17    400\n# … with 4 more rows\n\nThe following example performs a normality test on only a few selected variables.\n\n\n# Select columns by name\nnormality(carseats, Sales, CompPrice, Income)\n\n\n# A tibble: 3 x 4\n  vars      statistic      p_value sample\n  <chr>         <dbl>        <dbl>  <dbl>\n1 Sales         0.995 0.254           400\n2 CompPrice     0.998 0.977           400\n3 Income        0.961 0.0000000152    400\n\n\n# Select all columns between year and day (inclusive)\nnormality(carseats, Sales:Income)\n\n\n# A tibble: 3 x 4\n  vars      statistic      p_value sample\n  <chr>         <dbl>        <dbl>  <dbl>\n1 Sales         0.995 0.254           400\n2 CompPrice     0.998 0.977           400\n3 Income        0.961 0.0000000152    400\n\n\n# Select all columns except those from year to day (inclusive)\nnormality(carseats, -(Sales:Income))\n\n\n# A tibble: 5 x 4\n  vars        statistic  p_value sample\n  <chr>           <dbl>    <dbl>  <dbl>\n1 Advertising     0.874 1.49e-17    400\n2 Population      0.952 4.08e-10    400\n3 Price           0.996 3.90e- 1    400\n4 Age             0.957 1.86e- 9    400\n# … with 1 more row\n\nYou can use dplyr to sort variables that do not follow a normal distribution in order of p_value:\n\n\nlibrary(dplyr)\n\ncarseats %>%\n  normality() %>%\n  filter(p_value <= 0.01) %>% \n  arrange(abs(p_value))\n\n\n# A tibble: 5 x 4\n  vars        statistic  p_value sample\n  <chr>           <dbl>    <dbl>  <dbl>\n1 Advertising     0.874 1.49e-17    400\n2 Education       0.924 2.43e-13    400\n3 Population      0.952 4.08e-10    400\n4 Age             0.957 1.86e- 9    400\n# … with 1 more row\n\nIn particular, the Advertising variable is considered to be the most out of the normal distribution.\nThe normality() function supports the group_by() function syntax in the dplyr package.\n\n\ncarseats %>%\n  group_by(ShelveLoc, US) %>%\n  normality(Income) %>% \n  arrange(desc(p_value))\n\n\n# A tibble: 6 x 6\n  variable ShelveLoc US    statistic p_value sample\n  <chr>    <fct>     <fct>     <dbl>   <dbl>  <dbl>\n1 Income   Bad       No        0.969  0.470      34\n2 Income   Bad       Yes       0.958  0.0343     62\n3 Income   Good      No        0.902  0.0328     24\n4 Income   Good      Yes       0.955  0.0296     61\n# … with 2 more rows\n\nThe Income variable does not follow the normal distribution. However, the case where US is No and ShelveLoc is Good and Bad at the significance level of 0.01, it follows the normal distribution.\nThe following example performs normality test of log(Income) for each combination of ShelveLoc and US categorical variables to search for variables that follow the normal distribution.\n\n\ncarseats %>%\n  mutate(log_income = log(Income)) %>%\n  group_by(ShelveLoc, US) %>%\n  normality(log_income) %>%\n  filter(p_value > 0.01)\n\n\n# A tibble: 1 x 6\n  variable   ShelveLoc US    statistic p_value sample\n  <chr>      <fct>     <fct>     <dbl>   <dbl>  <dbl>\n1 log_income Bad       No        0.940  0.0737     34\n\nVisualization of normality of numerical variables using plot_normality()\nplot_normality() visualizes the normality of numeric data.\nThe information visualized by plot_normality() is as follows.:\nHistogram of original data\nQ-Q plot of original data\nhistogram of log transformed data\nHistogram of square root transformed data\nIn the data analysis process, it often encounters numerical data that follows the power-law distribution. Since the numerical data that follows the power-law distribution is converted into a normal distribution by performing the log or sqrt transformation, so draw a histogram of the log and sqrt transformed data.\nplot_normality() can also specify several variables like normality() function.\n\n\n# Select columns by name\nplot_normality(carseats, Sales, CompPrice)\n\n\n\n\nThe plot_normality() function also supports the group_by() function syntax in the dplyr package.\n\n\ncarseats %>%\n  filter(ShelveLoc == \"Good\") %>%\n  group_by(US) %>%\n  plot_normality(Income)\n\n\n\nEDA of bivariate data\nCalculation of correlation coefficient using correlate()\ncorrelate() calculates the correlation coefficient of all combinations of carseats numerical variables as follows:\n\n\ncorrelate(carseats)\n\n\n# A tibble: 56 x 3\n  var1        var2  coef_corr\n  <fct>       <fct>     <dbl>\n1 CompPrice   Sales    0.0641\n2 Income      Sales    0.151 \n3 Advertising Sales    0.270 \n4 Population  Sales    0.0505\n# … with 52 more rows\n\nThe following example performs a normality test only on combinations that include several selected variables.\n\n\n# Select columns by name\ncorrelate(carseats, Sales, CompPrice, Income)\n\n\n# A tibble: 21 x 3\n  var1      var2      coef_corr\n  <fct>     <fct>         <dbl>\n1 CompPrice Sales        0.0641\n2 Income    Sales        0.151 \n3 Sales     CompPrice    0.0641\n4 Income    CompPrice   -0.0761\n# … with 17 more rows\n\n\n# Select all columns between year and day (include)\ncorrelate(carseats, Sales:Income)\n\n\n# A tibble: 21 x 3\n  var1      var2      coef_corr\n  <fct>     <fct>         <dbl>\n1 CompPrice Sales        0.0641\n2 Income    Sales        0.151 \n3 Sales     CompPrice    0.0641\n4 Income    CompPrice   -0.0761\n# … with 17 more rows\n\n\n# Select all columns except those from year to day (exclude)\ncorrelate(carseats, -(Sales:Income))\n\n\n# A tibble: 35 x 3\n  var1        var2  coef_corr\n  <fct>       <fct>     <dbl>\n1 Advertising Sales    0.270 \n2 Population  Sales    0.0505\n3 Price       Sales   -0.445 \n4 Age         Sales   -0.232 \n# … with 31 more rows\n\ncorrelate() produces two pairs of variables. So the following example uses filter() to get the correlation coefficient for a pair of variable combinations:\n\n\ncarseats %>%\n  correlate(Sales:Income) %>%\n  filter(as.integer(var1) > as.integer(var2))\n\n\n# A tibble: 3 x 3\n  var1      var2      coef_corr\n  <fct>     <fct>         <dbl>\n1 CompPrice Sales        0.0641\n2 Income    Sales        0.151 \n3 Income    CompPrice   -0.0761\n\nThe correlate() also supports the group_by() function syntax in the dplyr package.\n\n\ncarseats %>%\n  filter(ShelveLoc == \"Good\") %>%\n  group_by(Urban, US) %>%\n  correlate(Sales) %>%\n  filter(abs(coef_corr) > 0.5)\n\n\n# A tibble: 10 x 5\n  Urban US    var1  var2       coef_corr\n  <fct> <fct> <fct> <fct>          <dbl>\n1 No    No    Sales Population    -0.530\n2 No    No    Sales Price         -0.838\n3 No    Yes   Sales Price         -0.630\n4 Yes   No    Sales Price         -0.833\n# … with 6 more rows\n\nVisualization of the correlation matrix using plot_correlate()\nplot_correlate() visualizes the correlation matrix.\n\n\nplot_correlate(carseats)\n\n\n\n\nplot_correlate() can also specify multiple variables, like the correlate() function. The following is a visualization of the correlation matrix including several selected variables.\n\n\n# Select columns by name\nplot_correlate(carseats, Sales, Price)\n\n\n\nThe plot_correlate() function also supports the group_by() function syntax in the dplyr package.\n\n\ncarseats %>%\n  filter(ShelveLoc == \"Good\") %>%\n  group_by(Urban) %>%\n  plot_correlate(Sales)\n\n\n\nEDA based on target variable\nDefinition of target variable\nTo perform EDA based on target variable, you need to create a target_by class object. target_by() creates a target_by class with an object inheriting data.frame or data.frame. target_by() is similar to group_by() in dplyr which creates grouped_df. The difference is that you specify only one variable.\nThe following is an example of specifying US as target variable in carseats data.frame.:\n\n\ncateg <- target_by(carseats, US)\n\n\n\nEDA when target variable is categorical variable\nLet’s perform EDA when the target variable is a categorical variable. When the categorical variable US is the target variable, we examine the relationship between the target variable and the predictor.\nCases where predictors are numeric variable\nrelate() shows the relationship between the target variable and the predictor. The following example shows the relationship between Sales and the target variable US. The predictor Sales is a numeric variable. In this case, the descriptive statistics are shown for each level of the target variable.\n\n\n# If the variable of interest is a numerical variable\ncat_num <- relate(categ, Sales)\ncat_num\n\n\n# A tibble: 3 x 27\n  variable US        n    na  mean    sd se_mean   IQR skewness\n  <chr>    <fct> <int> <int> <dbl> <dbl>   <dbl> <dbl>    <dbl>\n1 Sales    No      142     0  6.82  2.60   0.218  3.44   0.323 \n2 Sales    Yes     258     0  7.87  2.88   0.179  4.23   0.0760\n3 Sales    total   400     0  7.50  2.82   0.141  3.93   0.186 \n# … with 18 more variables: kurtosis <dbl>, p00 <dbl>, p01 <dbl>,\n#   p05 <dbl>, p10 <dbl>, p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>,\n#   p50 <dbl>, p60 <dbl>, p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>,\n#   p95 <dbl>, p99 <dbl>, p100 <dbl>\n\nsummary(cat_num)\n\n\n   variable             US          n               na   \n Length:3           No   :1   Min.   :142.0   Min.   :0  \n Class :character   Yes  :1   1st Qu.:200.0   1st Qu.:0  \n Mode  :character   total:1   Median :258.0   Median :0  \n                              Mean   :266.7   Mean   :0  \n                              3rd Qu.:329.0   3rd Qu.:0  \n                              Max.   :400.0   Max.   :0  \n      mean             sd           se_mean            IQR       \n Min.   :6.823   Min.   :2.603   Min.   :0.1412   Min.   :3.442  \n 1st Qu.:7.160   1st Qu.:2.713   1st Qu.:0.1602   1st Qu.:3.686  \n Median :7.496   Median :2.824   Median :0.1791   Median :3.930  \n Mean   :7.395   Mean   :2.768   Mean   :0.1796   Mean   :3.866  \n 3rd Qu.:7.682   3rd Qu.:2.851   3rd Qu.:0.1988   3rd Qu.:4.077  \n Max.   :7.867   Max.   :2.877   Max.   :0.2184   Max.   :4.225  \n    skewness          kurtosis             p00        \n Min.   :0.07603   Min.   :-0.32638   Min.   :0.0000  \n 1st Qu.:0.13080   1st Qu.:-0.20363   1st Qu.:0.0000  \n Median :0.18556   Median :-0.08088   Median :0.0000  \n Mean   :0.19489   Mean   : 0.13350   Mean   :0.1233  \n 3rd Qu.:0.25432   3rd Qu.: 0.36344   3rd Qu.:0.1850  \n Max.   :0.32308   Max.   : 0.80776   Max.   :0.3700  \n      p01              p05             p10             p20       \n Min.   :0.4675   Min.   :3.147   Min.   :3.917   Min.   :4.754  \n 1st Qu.:0.6868   1st Qu.:3.148   1st Qu.:4.018   1st Qu.:4.910  \n Median :0.9062   Median :3.149   Median :4.119   Median :5.066  \n Mean   :1.0072   Mean   :3.183   Mean   :4.073   Mean   :5.051  \n 3rd Qu.:1.2771   3rd Qu.:3.200   3rd Qu.:4.152   3rd Qu.:5.199  \n Max.   :1.6480   Max.   :3.252   Max.   :4.184   Max.   :5.332  \n      p25             p30             p40             p50       \n Min.   :5.080   Min.   :5.306   Min.   :5.994   Min.   :6.660  \n 1st Qu.:5.235   1st Qu.:5.587   1st Qu.:6.301   1st Qu.:7.075  \n Median :5.390   Median :5.867   Median :6.608   Median :7.490  \n Mean   :5.411   Mean   :5.775   Mean   :6.506   Mean   :7.313  \n 3rd Qu.:5.576   3rd Qu.:6.010   3rd Qu.:6.762   3rd Qu.:7.640  \n Max.   :5.763   Max.   :6.153   Max.   :6.916   Max.   :7.790  \n      p60             p70             p75             p80        \n Min.   :7.496   Min.   :7.957   Min.   :8.523   Min.   : 8.772  \n 1st Qu.:7.787   1st Qu.:8.386   1st Qu.:8.921   1st Qu.: 9.265  \n Median :8.078   Median :8.815   Median :9.320   Median : 9.758  \n Mean   :8.076   Mean   :8.740   Mean   :9.277   Mean   : 9.665  \n 3rd Qu.:8.366   3rd Qu.:9.132   3rd Qu.:9.654   3rd Qu.:10.111  \n Max.   :8.654   Max.   :9.449   Max.   :9.988   Max.   :10.464  \n      p90              p95             p99             p100      \n Min.   : 9.349   Min.   :11.28   Min.   :13.64   Min.   :14.90  \n 1st Qu.:10.325   1st Qu.:11.86   1st Qu.:13.78   1st Qu.:15.59  \n Median :11.300   Median :12.44   Median :13.91   Median :16.27  \n Mean   :10.795   Mean   :12.08   Mean   :13.86   Mean   :15.81  \n 3rd Qu.:11.518   3rd Qu.:12.49   3rd Qu.:13.97   3rd Qu.:16.27  \n Max.   :11.736   Max.   :12.54   Max.   :14.03   Max.   :16.27  \n\nplot() visualizes the relate class object created by relate() as the relationship between the target variable and the predictor variable. The relationship between US and Sales is visualized by density plot.\n\n\nplot(cat_num)\n\n\n\n\nCases where predictors are categorical variable\nThe following example shows the relationship between ShelveLoc and the target variable US. The predictor variable ShelveLoc is a categorical variable. In this case, it shows the contingency table of two variables. The summary() function performs independence test on the contingency table.\n\n\n# If the variable of interest is a categorical variable\ncat_cat <- relate(categ, ShelveLoc)\ncat_cat\n\n\n     ShelveLoc\nUS    Bad Good Medium\n  No   34   24     84\n  Yes  62   61    135\n\nsummary(cat_cat)\n\n\nCall: xtabs(formula = formula_str, data = data, addNA = TRUE)\nNumber of cases in table: 400 \nNumber of factors: 2 \nTest for independence of all factors:\n    Chisq = 2.7397, df = 2, p-value = 0.2541\n\nplot() visualizes the relationship between the target variable and the predictor. The relationship between US and ShelveLoc is represented by a mosaics plot.\n\n\nplot(cat_cat)\n\n\n\n\nEDA when target variable is numerical variable\nLet’s perform EDA when the target variable is numeric. When the numeric variable Sales is the target variable, we examine the relationship between the target variable and the predictor.\n\n\n# If the variable of interest is a numerical variable\nnum <- target_by(carseats, Sales)\n\n\n\nCases where predictors are numeric variable\nThe following example shows the relationship between Price and the target variable Sales. The predictor variable Price is a numeric variable. In this case, it shows the result of a simple linear model of the target ~ predictor formula. The summary() function expresses the details of the model.\n\n\n# If the variable of interest is a numerical variable\nnum_num <- relate(num, Price)\nnum_num\n\n\n\nCall:\nlm(formula = formula_str, data = data)\n\nCoefficients:\n(Intercept)        Price  \n   13.64192     -0.05307  \n\nsummary(num_num)\n\n\n\nCall:\nlm(formula = formula_str, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5224 -1.8442 -0.1459  1.6503  7.5108 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.641915   0.632812  21.558   <2e-16 ***\nPrice       -0.053073   0.005354  -9.912   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.532 on 398 degrees of freedom\nMultiple R-squared:  0.198, Adjusted R-squared:  0.196 \nF-statistic: 98.25 on 1 and 398 DF,  p-value: < 2.2e-16\n\nplot() visualizes the relationship between the target and predictor variables. The relationship between Sales and Price is visualized with a scatter plot. The figure on the left shows the scatter plot of Sales and Price and the confidence interval of the regression line and regression line. The figure on the right shows the relationship between the original data and the predicted values of the linear model as a scatter plot. If there is a linear relationship between the two variables, the scatter plot of the observations converges on the red diagonal line.\n\n\nplot(num_num)\n\n\n\n\nThe scatter plot of the data with a large number of observations is output as overlapping points. This makes it difficult to judge the relationship between the two variables. It also takes a long time to perform the visualization. In this case, the above problem can be solved by hexabin plot.\nIn plot(), the hex_thres argument provides a basis for drawing hexabin plot. If the number of observations is greater than hex_thres, draw a hexabin plot.\nThe following example visualizes the hexabin plot rather than the scatter plot by specifying 350 for the hex_thres argument. This is because the number of observations is 400.\n\n\nplot(num_num, hex_thres = 350)\n\n\n\n\nCases where predictors are categorical variable\nThe following example shows the relationship between ShelveLoc and the target variable Sales. The predictor ShelveLoc is a categorical variable and shows the result of one-way ANOVA of target ~ predictor relationship. The results are expressed in terms of ANOVA. The summary() function shows the regression coefficients for each level of the predictor. In other words, it shows detailed information about simple regression analysis of target ~ predictor relationship.\n\n\n# If the variable of interest is a categorical variable\nnum_cat <- relate(num, ShelveLoc)\nnum_cat\n\n\nAnalysis of Variance Table\n\nResponse: Sales\n           Df Sum Sq Mean Sq F value    Pr(>F)    \nShelveLoc   2 1009.5  504.77   92.23 < 2.2e-16 ***\nResiduals 397 2172.7    5.47                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(num_cat)\n\n\n\nCall:\nlm(formula = formula(formula_str), data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.3066 -1.6282 -0.0416  1.5666  6.1471 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       5.5229     0.2388  23.131  < 2e-16 ***\nShelveLocGood     4.6911     0.3484  13.464  < 2e-16 ***\nShelveLocMedium   1.7837     0.2864   6.229  1.2e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.339 on 397 degrees of freedom\nMultiple R-squared:  0.3172,    Adjusted R-squared:  0.3138 \nF-statistic: 92.23 on 2 and 397 DF,  p-value: < 2.2e-16\n\nplot() visualizes the relationship between the target variable and the predictor. The relationship between Sales and ShelveLoc is represented by a box plot.\n\n\nplot(num_cat)\n\n\n\n\nAutomated report\ndlookr provides two automated EDA reports:\nWeb page-based dynamic reports can perform in-depth analysis through visualization and statistical tables.\nStatic reports generated as pdf files or html files can be archived as output of data analysis.\nCreate a dynamic report using eda_web_report()\neda_web_report() create dynamic report for object inherited from data.frame(tbl_df, tbl, etc) or data.frame.\nContents of dynamic web report\nThe contents of the report are as follows.:\nOverview\nData Structures\nData Types\nJob Informations\n\nUnivariate Analysis\nDescriptive Statistics\nNormality Test\n\nBivariate Analysis\nCompare Numerical Variables\nCompare Categorical Variables\n\nMultivariate Analysis\nCorrelation Analysis\nCorrelation Matrix\nCorrelation Plot\n\n\nTarget based Analysis\nGrouped Numerical Variables\nGrouped Categorical Variables\nGrouped Correlation\n\nSome arguments for dynamic web report\neda_web_report() generates various reports with the following arguments.\ntarget\ntarget variable\n\noutput_file\nname of generated file.\n\noutput_dir\nname of directory to generate report file.\n\ntitle\ntitle of report.\n\nsubtitle\nsubtitle of report.\n\nauthor\nauthor of report.\n\ntitle_color\ncolor of title.\n\nlogo_img\nname of logo image file on top left.\n\ncreate_date\nThe date on which the report is generated.\n\ntheme\nname of theme for report. support “orange” and “blue”.\n\nsample_percent\nSample percent of data for performing EDA.\n\nThe following script creates a EDA report for the data.frame class object, heartfailure.\n\n\nheartfailure %>%\n  eda_web_report(target = \"death_event\", subtitle = \"heartfailure\", \n                 output_dir = \"./\", output_file = \"EDA.html\", theme = \"blue\")\n\n\n\nScreenshot of dynamic report\nThe dynamic contents of the report is shown in the following figure.:\n\n\n\n(#fig:eda_web_title)The part of the report\n\n\n\nCreate a EDA report using eda_paged_report()\neda_paged_report() create static report for object inherited from data.frame(tbl_df, tbl, etc) or data.frame.\nContents of static paged report\nThe contents of the report are as follows.:\nOverview\nData Structures\nJob Informations\n\nUnivariate Analysis\nDescriptive Statistics\nNumerical Variables\nCategorical Variables\n\nNormality Test\n\nBivariate Analysis\nCompare Numerical Variables\nCompare Categorical Variables\n\nMultivariate Analysis\nCorrelation Analysis\nCorrelation Coefficient Matrix\nCorrelation Plot\n\n\nTarget based Analysis\nGrouped Numerical Variables\nGrouped Categorical Variables\nGrouped Correlation\n\nSome arguments for static paged report\neda_paged_report() generates various reports with the following arguments.\ntarget\ntarget variable\n\noutput_format\nreport output type. Choose either “pdf” and “html”.\n\noutput_file\nname of generated file.\n\noutput_dir\nname of directory to generate report file.\n\ntitle\ntitle of report.\n\nsubtitle\nsubtitle of report.\n\nabstract_title\nabstract of report\n\nauthor\nauthor of report.\n\ntitle_color\ncolor of title.\n\nsubtitle_color\ncolor of subtitle.\n\nlogo_img\nname of logo image file on top left.\n\ncover_img\nname of cover image file on center.\n\ncreate_date\nThe date on which the report is generated.\n\ntheme\nname of theme for report. support “orange” and “blue”.\n\nsample_percent\nSample percent of data for performing EDA.\n\nThe following script creates a EDA report for the data.frame class object, heartfailure.\n\n\nheartfailure %>%\n  eda_paged_report(target = \"death_event\", subtitle = \"heartfailure\", \n                   output_dir = \"./\", output_file = \"EDA.pdf\", theme = \"blue\")\n\n\n\nScreenshot of static report\nThe cover of the report is shown in the following figure.:\n\n\n\n(#fig:eda_paged_cover)The part of the report\n\n\n\nThe contents of the report is shown in the following figure.:\n\n\n\n(#fig:eda_paged_cntent)The dynamic contents of the report\n\n\n\nExploratory data analysis for tables in DBMS\nEDA function for table of DBMS supports In-database mode that performs SQL operations on the DBMS side. If the size of the data is large, using In-database mode is faster.\nIt is difficult to obtain anomaly or to implement the sampling-based algorithm in SQL of DBMS. So some functions do not yet support In-database mode. In this case, it is performed in In-memory mode in which table data is brought to R side and calculated. In this case, if the data size is large, the execution speed may be slow. It supports the collect_size argument, which allows you to import the specified number of samples of data into R.\nIn-database support functions\nnone\n\nIn-database not support functions\nnormality()\nplot_normality()\ncorrelate()\nplot_correlate()\ndescribe()\neda_web_report()\neda_paged_report()\n\nPreparing table data\nCopy the carseats data frame to the SQLite DBMS and create it as a table named TB_CARSEATS. Mysql/MariaDB, PostgreSQL, Oracle DBMS, other DBMS are also available for your environment.\n\n\nif (!require(DBI)) install.packages('DBI')\nif (!require(RSQLite)) install.packages('RSQLite')\nif (!require(dplyr)) install.packages('dplyr')\nif (!require(dbplyr)) install.packages('dbplyr')\n\nlibrary(dplyr)\n\ncarseats <- ISLR::Carseats\ncarseats[sample(seq(NROW(carseats)), 20), \"Income\"] <- NA\ncarseats[sample(seq(NROW(carseats)), 5), \"Urban\"] <- NA\n\n# connect DBMS\ncon_sqlite <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\n\n# copy carseats to the DBMS with a table named TB_CARSEATS\ncopy_to(con_sqlite, carseats, name = \"TB_CARSEATS\", overwrite = TRUE)\n\n\n\nCalculating descriptive statistics of numerical column of table in the DBMS\nUse dplyr::tbl() to create a tbl_dbi object, then use it as a data frame object. That is, the data argument of all EDA function is specified as tbl_dbi object instead of data frame object.\n\n\n# Positive values select variables\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  describe(Sales, CompPrice, Income)\n\n\n# A tibble: 3 x 26\n  variable      n    na   mean    sd se_mean   IQR skewness kurtosis\n  <chr>     <int> <int>  <dbl> <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n1 Sales       400     0   7.50  2.82   0.141  3.93   0.186   -0.0809\n2 CompPrice   400     0 125.   15.3    0.767 20     -0.0428   0.0417\n3 Income      380    20  68.8  28.0    1.44  47.2    0.0641  -1.08  \n# … with 17 more variables: p00 <dbl>, p01 <dbl>, p05 <dbl>,\n#   p10 <dbl>, p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>, p50 <dbl>,\n#   p60 <dbl>, p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>, p95 <dbl>,\n#   p99 <dbl>, p100 <dbl>\n\n\n# Negative values to drop variables, and In-memory mode and collect size is 200\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  describe(-Sales, -CompPrice, -Income, collect_size = 200)\n\n\n# A tibble: 5 x 26\n  variable       n    na   mean     sd se_mean   IQR skewness kurtosis\n  <chr>      <int> <int>  <dbl>  <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n1 Advertisi…   200     0   5.88   6.07   0.429  11     0.648    -0.667\n2 Population   200     0 255.   149.    10.6   251.    0.0241   -1.22 \n3 Price        200     0 114.    23.7    1.68   31.2  -0.107     1.08 \n4 Age          200     0  54.6   15.9    1.13   24    -0.245    -1.01 \n# … with 1 more row, and 17 more variables: p00 <dbl>, p01 <dbl>,\n#   p05 <dbl>, p10 <dbl>, p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>,\n#   p50 <dbl>, p60 <dbl>, p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>,\n#   p95 <dbl>, p99 <dbl>, p100 <dbl>\n\n\n# Find the statistic of all numerical variables by 'ShelveLoc' and 'US',\n# and extract only those with 'ShelveLoc' variable level is \"Good\".\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  group_by(ShelveLoc, US) %>%\n  describe() %>%\n  filter(ShelveLoc == \"Good\")\n\n\n# A tibble: 16 x 28\n  variable    ShelveLoc US        n    na    mean     sd se_mean   IQR\n  <chr>       <chr>     <chr> <int> <int>   <dbl>  <dbl>   <dbl> <dbl>\n1 Advertising Good      No       24     0  0.0417  0.204  0.0417     0\n2 Advertising Good      Yes      61     0 10.2     5.91   0.757      7\n3 Age         Good      No       24     0 52.3    17.2    3.52      26\n4 Age         Good      Yes      61     0 52.7    14.8    1.90      22\n# … with 12 more rows, and 19 more variables: skewness <dbl>,\n#   kurtosis <dbl>, p00 <dbl>, p01 <dbl>, p05 <dbl>, p10 <dbl>,\n#   p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>, p50 <dbl>, p60 <dbl>,\n#   p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>, p95 <dbl>, p99 <dbl>,\n#   p100 <dbl>\n\n\n# extract only those with 'Urban' variable level is \"Yes\",\n# and find 'Sales' statistics by 'ShelveLoc' and 'US'\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  filter(Urban == \"Yes\") %>%\n  group_by(ShelveLoc, US) %>%\n  describe(Sales)\n\n\n# A tibble: 6 x 28\n  variable ShelveLoc US        n    na  mean    sd se_mean   IQR\n  <chr>    <chr>     <chr> <int> <int> <dbl> <dbl>   <dbl> <dbl>\n1 Sales    Bad       No       23     0  5.36  1.91   0.398  2.32\n2 Sales    Bad       Yes      50     0  5.54  2.57   0.364  3.74\n3 Sales    Good      No       18     0  9.21  2.97   0.700  3.71\n4 Sales    Good      Yes      37     0 10.9   2.37   0.389  3.41\n# … with 2 more rows, and 19 more variables: skewness <dbl>,\n#   kurtosis <dbl>, p00 <dbl>, p01 <dbl>, p05 <dbl>, p10 <dbl>,\n#   p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>, p50 <dbl>, p60 <dbl>,\n#   p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>, p95 <dbl>, p99 <dbl>,\n#   p100 <dbl>\n\nTest of normality on numeric columns using in the DBMS\n\n\n# Test all numerical variables by 'ShelveLoc' and 'US',\n# and extract only those with 'ShelveLoc' variable level is \"Good\".\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n group_by(ShelveLoc, US) %>%\n normality() %>%\n filter(ShelveLoc == \"Good\")\n\n\n# A tibble: 16 x 6\n  variable  ShelveLoc US    statistic p_value sample\n  <chr>     <chr>     <chr>     <dbl>   <dbl>  <dbl>\n1 Sales     Good      No        0.955   0.342     24\n2 Sales     Good      Yes       0.983   0.567     61\n3 CompPrice Good      No        0.970   0.658     24\n4 CompPrice Good      Yes       0.984   0.598     61\n# … with 12 more rows\n\n\n# extract only those with 'Urban' variable level is \"Yes\",\n# and test 'Sales' by 'ShelveLoc' and 'US'\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n filter(Urban == \"Yes\") %>%\n group_by(ShelveLoc, US) %>%\n normality(Sales)\n\n\n# A tibble: 6 x 6\n  variable ShelveLoc US    statistic p_value sample\n  <chr>    <chr>     <chr>     <dbl>   <dbl>  <dbl>\n1 Sales    Bad       No        0.985   0.968     23\n2 Sales    Bad       Yes       0.985   0.774     50\n3 Sales    Good      No        0.959   0.576     18\n4 Sales    Good      Yes       0.969   0.384     37\n# … with 2 more rows\n\n\n# Test log(Income) variables by 'ShelveLoc' and 'US',\n# and extract only p.value greater than 0.01.\n\n# SQLite extension functions for log transformation\nRSQLite::initExtension(con_sqlite)\n\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n mutate(log_income = log(Income)) %>%\n group_by(ShelveLoc, US) %>%\n normality(log_income) %>%\n filter(p_value > 0.01)\n\n\n# A tibble: 1 x 6\n  variable   ShelveLoc US    statistic p_value sample\n  <chr>      <chr>     <chr>     <dbl>   <dbl>  <dbl>\n1 log_income Bad       No        0.946   0.104     34\n\nNormalization visualization of numerical column in the DBMS\n\n\n# extract only those with 'ShelveLoc' variable level is \"Good\",\n# and plot 'Income' by 'US'\n# the result is same as a data.frame, but not display here. reference above in document.\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  filter(ShelveLoc == \"Good\") %>%\n  group_by(US) %>%\n  plot_normality(Income)\n\n\n\nCompute the correlation coefficient between two columns of table in DBMS\n\n\n# Correlation coefficient\n# that eliminates redundant combination of variables\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  correlate() %>%\n  filter(as.integer(var1) > as.integer(var2))\n\n\n# A tibble: 28 x 3\n  var1        var2  coef_corr\n  <fct>       <fct>     <dbl>\n1 CompPrice   Sales    0.0641\n2 Income      Sales    0.141 \n3 Advertising Sales    0.270 \n4 Population  Sales    0.0505\n# … with 24 more rows\n\n\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  correlate(Sales, Price) %>%\n  filter(as.integer(var1) > as.integer(var2))\n\n\n# A tibble: 5 x 3\n  var1  var2        coef_corr\n  <fct> <fct>           <dbl>\n1 Price Sales         -0.445 \n2 Price CompPrice      0.585 \n3 Price Income        -0.0484\n4 Price Advertising    0.0445\n# … with 1 more row\n\n\n# Compute the correlation coefficient of Sales variable by 'ShelveLoc'\n# and 'US' variables. And extract only those with absolute\n# value of correlation coefficient is greater than 0.5\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  group_by(ShelveLoc, US) %>%\n  correlate(Sales) %>%\n  filter(abs(coef_corr) >= 0.5)\n\n\n# A tibble: 6 x 5\n  ShelveLoc US    var1  var2  coef_corr\n  <chr>     <chr> <fct> <fct>     <dbl>\n1 Bad       No    Sales Price    -0.527\n2 Bad       Yes   Sales Price    -0.583\n3 Good      No    Sales Price    -0.811\n4 Good      Yes   Sales Price    -0.603\n# … with 2 more rows\n\n\n# extract only those with 'ShelveLoc' variable level is \"Good\",\n# and compute the correlation coefficient of 'Sales' variable\n# by 'Urban' and 'US' variables.\n# And the correlation coefficient is negative and smaller than 0.5\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  filter(ShelveLoc == \"Good\") %>%\n  group_by(Urban, US) %>%\n  correlate(Sales) %>%\n  filter(coef_corr < 0) %>%\n  filter(abs(coef_corr) > 0.5)\n\n\n# A tibble: 10 x 5\n  Urban US    var1  var2       coef_corr\n  <chr> <chr> <fct> <fct>          <dbl>\n1 No    No    Sales Population    -0.530\n2 No    No    Sales Price         -0.838\n3 No    Yes   Sales Price         -0.644\n4 Yes   No    Sales Price         -0.833\n# … with 6 more rows\n\nVisualize correlation plot of numerical columns in the DBMS\n\n\n# Extract only those with 'ShelveLoc' variable level is \"Good\",\n# and visualize correlation plot of 'Sales' variable by 'Urban'\n# and 'US' variables.\n# the result is same as a data.frame, but not display here. reference above in document.\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  filter(ShelveLoc == \"Good\") %>%\n  group_by(Urban) %>%\n  plot_correlate(Sales)\n\n\n\nEDA based on target variable\nThe following is an EDA where the target column is character and the predictor column is a numeric type.\n\n\n# If the target variable is a categorical variable\ncateg <- target_by(con_sqlite %>% tbl(\"TB_CARSEATS\") , US)\n\n# If the variable of interest is a numerical variable\ncat_num <- relate(categ, Sales)\ncat_num\n\n\n# A tibble: 3 x 27\n  variable US        n    na  mean    sd se_mean   IQR skewness\n  <chr>    <fct> <int> <int> <dbl> <dbl>   <dbl> <dbl>    <dbl>\n1 Sales    No      142     0  6.82  2.60   0.218  3.44   0.323 \n2 Sales    Yes     258     0  7.87  2.88   0.179  4.23   0.0760\n3 Sales    total   400     0  7.50  2.82   0.141  3.93   0.186 \n# … with 18 more variables: kurtosis <dbl>, p00 <dbl>, p01 <dbl>,\n#   p05 <dbl>, p10 <dbl>, p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>,\n#   p50 <dbl>, p60 <dbl>, p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>,\n#   p95 <dbl>, p99 <dbl>, p100 <dbl>\n\nsummary(cat_num)\n\n\n   variable             US          n               na   \n Length:3           No   :1   Min.   :142.0   Min.   :0  \n Class :character   Yes  :1   1st Qu.:200.0   1st Qu.:0  \n Mode  :character   total:1   Median :258.0   Median :0  \n                              Mean   :266.7   Mean   :0  \n                              3rd Qu.:329.0   3rd Qu.:0  \n                              Max.   :400.0   Max.   :0  \n      mean             sd           se_mean            IQR       \n Min.   :6.823   Min.   :2.603   Min.   :0.1412   Min.   :3.442  \n 1st Qu.:7.160   1st Qu.:2.713   1st Qu.:0.1602   1st Qu.:3.686  \n Median :7.496   Median :2.824   Median :0.1791   Median :3.930  \n Mean   :7.395   Mean   :2.768   Mean   :0.1796   Mean   :3.866  \n 3rd Qu.:7.682   3rd Qu.:2.851   3rd Qu.:0.1988   3rd Qu.:4.077  \n Max.   :7.867   Max.   :2.877   Max.   :0.2184   Max.   :4.225  \n    skewness          kurtosis             p00        \n Min.   :0.07603   Min.   :-0.32638   Min.   :0.0000  \n 1st Qu.:0.13080   1st Qu.:-0.20363   1st Qu.:0.0000  \n Median :0.18556   Median :-0.08088   Median :0.0000  \n Mean   :0.19489   Mean   : 0.13350   Mean   :0.1233  \n 3rd Qu.:0.25432   3rd Qu.: 0.36344   3rd Qu.:0.1850  \n Max.   :0.32308   Max.   : 0.80776   Max.   :0.3700  \n      p01              p05             p10             p20       \n Min.   :0.4675   Min.   :3.147   Min.   :3.917   Min.   :4.754  \n 1st Qu.:0.6868   1st Qu.:3.148   1st Qu.:4.018   1st Qu.:4.910  \n Median :0.9062   Median :3.149   Median :4.119   Median :5.066  \n Mean   :1.0072   Mean   :3.183   Mean   :4.073   Mean   :5.051  \n 3rd Qu.:1.2771   3rd Qu.:3.200   3rd Qu.:4.152   3rd Qu.:5.199  \n Max.   :1.6480   Max.   :3.252   Max.   :4.184   Max.   :5.332  \n      p25             p30             p40             p50       \n Min.   :5.080   Min.   :5.306   Min.   :5.994   Min.   :6.660  \n 1st Qu.:5.235   1st Qu.:5.587   1st Qu.:6.301   1st Qu.:7.075  \n Median :5.390   Median :5.867   Median :6.608   Median :7.490  \n Mean   :5.411   Mean   :5.775   Mean   :6.506   Mean   :7.313  \n 3rd Qu.:5.576   3rd Qu.:6.010   3rd Qu.:6.762   3rd Qu.:7.640  \n Max.   :5.763   Max.   :6.153   Max.   :6.916   Max.   :7.790  \n      p60             p70             p75             p80        \n Min.   :7.496   Min.   :7.957   Min.   :8.523   Min.   : 8.772  \n 1st Qu.:7.787   1st Qu.:8.386   1st Qu.:8.921   1st Qu.: 9.265  \n Median :8.078   Median :8.815   Median :9.320   Median : 9.758  \n Mean   :8.076   Mean   :8.740   Mean   :9.277   Mean   : 9.665  \n 3rd Qu.:8.366   3rd Qu.:9.132   3rd Qu.:9.654   3rd Qu.:10.111  \n Max.   :8.654   Max.   :9.449   Max.   :9.988   Max.   :10.464  \n      p90              p95             p99             p100      \n Min.   : 9.349   Min.   :11.28   Min.   :13.64   Min.   :14.90  \n 1st Qu.:10.325   1st Qu.:11.86   1st Qu.:13.78   1st Qu.:15.59  \n Median :11.300   Median :12.44   Median :13.91   Median :16.27  \n Mean   :10.795   Mean   :12.08   Mean   :13.86   Mean   :15.81  \n 3rd Qu.:11.518   3rd Qu.:12.49   3rd Qu.:13.97   3rd Qu.:16.27  \n Max.   :11.736   Max.   :12.54   Max.   :14.03   Max.   :16.27  \n\n\n\n# the result is same as a data.frame, but not display here. reference above in document.\nplot(cat_num)\n\n\n\nReporting the information of EDA for table of the DBMS\nThe following shows several examples of creating an EDA report for a DBMS table.\nUsing the collect_size argument, you can perform EDA with the corresponding number of sample data. If the number of data is very large, use collect_size.\n\n\n# create web report file. \ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  eda_web_report()\n  \n# create pdf file. file name is EDA.pdf, and collect size is 350\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  eda_paged_report(collect_size = 350, output_file = \"EDA.pdf\")\n\n\n\n\n\n\n",
      "last_modified": "2021-11-28T14:59:28+09:00"
    },
    {
      "path": "index.html",
      "title": "Introduce dlookr",
      "description": "Introduce dlookr package for data diagnosis, EDA, data transformation\n",
      "author": [
        {
          "name": "Choonghyun Ryu",
          "url": "https://dataholic.netlify.app/"
        }
      ],
      "date": "2021-11-28",
      "contents": "\n\nContents\nPreface\nSupported data structures\nList of supported tasks of data analytics\nDiagnose Data\nEDA\nTransform Data\nMiscellaneous\n\n\n\n\n\nPreface\nAfter you have acquired the data, you should do the following:\nDiagnose data quality.\nIf there is a problem with data quality,\nThe data must be corrected or re-acquired.\n\nExplore data to understand the data and find scenarios for performing the analysis.\nDerive new variables or perform variable transformations.\nThe dlookr package makes these steps fast and easy:\nPerforms a data diagnosis or automatically generates a data diagnosis report.\nDiscover data in a variety of ways, and automatically generate EDA(exploratory data analysis) report.\nImpute missing values and outliers, resolve skewed data, and binaries continuous variables into categorical variables. And generates an automated report to support it.\ndlookr increases synergy with dplyr. Particularly in data exploration and data wrangle, it increases the efficiency of the tidyverse package group.\nSupported data structures\nData diagnosis supports the following data structures.\ndata frame : data.frame class.\ndata table : tbl_df class.\ntable of DBMS : table of the DBMS through tbl_dbi.\nUse dplyr as the back-end interface for any DBI-compatible database.\n\nList of supported tasks of data analytics\nDiagnose Data\nOverall Diagnose Data\nTasks\nDescriptions\nFunctions\nSupport DBI\ndescribe overview of data\nInquire basic information to understand the data in general\noverview()\n\nsummary overview object\nsummary described overview of data\nsummary.overview()\n\nplot overview object\nplot described overview of data\nplot.overview()\n\ndiagnose data quality of variables\nThe scope of data quality diagnosis is information on missing values and unique value information\ndiagnose()\nx\ndiagnose data quality of categorical variables\nfrequency, ratio, rank by levels of each variables\ndiagnose_category()\nx\ndiagnose data quality of numerical variables\ndescriptive statistics, number of zero, minus, outliers\ndiagnose_numeric()\nx\ndiagnose data quality for outlier\nnumber of outliers, ratio, mean of outliers, mean with outliers, mean without outliers\ndiagnose_outlier()\nx\nplot outliers information of numerical data\nbox plot and histogram whith outliers, without outliers\nplot_outlier.data.frame()\nx\nplot outliers information of numerical data by target variable\nbox plot and density plot whith outliers, without outliers\nplot_outlier.target_df()\nx\ndiagnose combination of categorical variables\nCheck for sparse cases of level combinations of categorical variables\ndiagnose_sparese()\n\nVisualize Missing Values\nTasks\nDescriptions\nFunctions\nSupport DBI\npareto chart for missing value\nvisualize pareto chart for variables with missing value.\nplot_na_pareto()\n\ncombination chart for missing value\nvisualize distribution of missing value by combination of variables.\nplot_na_hclust()\n\nplot the combination variables that is include missing value\nvisualize the combinations of missing value across cases..\nplot_na_intersect()\n\nReporting\nTypes\nDescriptions\nFunctions\nSupport DBI\nreporting the information of data diagnosis into pdf file\nreport the information for diagnosing the quality of the data.\ndiagnose_report()\nx\nreporting the information of data diagnosis into html file\nreport the information for diagnosing the quality of the data.\ndiagnose_report()\nx\nreporting the information of data diagnosis into html file\ndynamic report the information for diagnosing the quality of the data.\ndiagnose_web_report()\nx\nreporting the information of data diagnosis into pdf and html file\npaged report the information for diagnosing the quality of the data.\ndiagnose_paged_report()\nx\nEDA\nUnivariate EDA\nTypes\nTasks\nDescriptions\nFunctions\nSupport DBI\ncategorical\nsummaries\nfrequency tables\nunivar_category()\n\ncategorical\nsummaries\nchi-squared test\nsummary.univar_category()\n\ncategorical\nvisualize\nbar charts\nplot.univar_category()\n\ncategorical\nvisualize\nbar charts\nplot_bar_category()\n\nnumerical\nsummaries\ndescriptive statistics\ndescribe()\nx\nnumerical\nsummaries\ndescriptive statistics\nunivar_numeric()\n\nnumerical\nsummaries\ndescriptive statistics of standardized variable\nsummary.univar_numeric()\n\nnumerical\nvisualize\nhistogram, box plot\nplot.univar_numeric()\n\nnumerical\nvisualize\nQ-Q plots\nplot_qq_numeric()\n\nnumerical\nvisualize\nbox plot\nplot_box_numeric()\n\nnumerical\nvisualize\nhistogram\nplot_hist_numeric()\n\nBivariate EDA\nTypes\nTasks\nDescriptions\nFunctions\nSupport DBI\ncategorical\nsummaries\nfrequency tables cross cases\ncompare_category()\n\ncategorical\nsummaries\ncontingency tables, chi-squared test\nsummary.compare_category()\n\ncategorical\nvisualize\nmosaics plot\nplot.compare_category()\n\nnumerical\nsummaries\ncorrelation coefficient, linear model summaries\ncompare_numeric()\n\nnumerical\nsummaries\ncorrelation coefficient, linear model summaries with threshold\nsummary.compare_numeric()\n\nnumerical\nvisualize\nscatter plot with marginal box plot\nplot.compare_numeric()\n\nnumerical\nCorrelate\ncorrelation coefficient\ncorrelate()\nx\nnumerical\nCorrelate\nvisualization of a correlation matrix\nplot_correlate()\nx\nNormality Test\nTypes\nTasks\nDescriptions\nFunctions\nSupport DBI\nnumerical\nsummaries\nShapiro-Wilk normality test\nnormality()\nx\nnumerical\nsummaries\nnormality diagnosis plot (histogram, Q-Q plots)\nplot_normality()\nx\nRelationship between target variable and predictors\nTarget Variable\nPredictor\nDescriptions\nFunctions\nSupport DBI\ncategorical\ncategorical\ncontingency tables\nrelate()\nx\ncategorical\ncategorical\nmosaics plot\nplot.relate()\nx\ncategorical\nnumerical\ndescriptive statistic for each levels and total observation\nrelate()\nx\ncategorical\nnumerical\ndensity plot\nplot.relate()\nx\ncategorical\ncategorical\nbar charts\nplot_bar_category()\n\nnumerical\ncategorical\nANOVA test\nrelate()\nx\nnumerical\ncategorical\nscatter plot\nplot.relate()\nx\nnumerical\nnumerical\nsimple linear model\nrelate()\nx\nnumerical\nnumerical\nbox plot\nplot.relate()\nx\ncategorical\nnumerical\nQ-Q plots\nplot_qq_numeric()\n\ncategorical\nnumerical\nbox plot\nplot_box_numeric()\n\ncategorical\nnumerical\nhistogram\nplot_hist_numeric()\n\nReporting\nTypes\nDescriptions\nFunctions\nSupport DBI\nreporting the information of EDA into pdf file\nreporting the information of EDA.\neda_report()\nx\nreporting the information of EDA into html file\nreporting the information of EDA.\neda_report()\nx\nreporting the information of EDA into pdf file\ndynamic reporting the information of EDA.\neda_web_report()\nx\nreporting the information of EDA into html file\npaged reporting the information of EDA.\neda_paged_report()\nx\nTransform Data\nFind Variables\nTypes\nDescriptions\nFunctions\nSupport DBI\nmissing values\nfind the variable that contains the missing value in the object that inherits the data.frame\nfind_na()\n\noutliers\nfind the numerical variable that contains outliers in the object that inherits the data.frame\nfind_outliers()\n\nskewed variable\nfind the numerical variable that skewed variable that inherits the data.frame\nfind_skewness()\n\nImputation\nTypes\nDescriptions\nFunctions\nSupport DBI\nmissing values\nmissing values are imputed with some representative values and statistical methods.\nimputate_na()\n\noutliers\noutliers are imputed with some representative values and statistical methods.\nimputate_outlier()\n\nsummaries\ncalculate descriptive statistics of the original and imputed values.\nsummary.imputation()\n\nvisualize\nthe imputation of a numerical variable is a density plot, and the imputation of a categorical variable is a bar plot.\nplot.imputation()\n\nBinning\nTypes\nDescriptions\nFunctions\nSupport DBI\nbinning\nconverts a numeric variable to a categorization variable\nbinning()\n\nsummaries\ncalculate frequency and relative frequency for each levels(bins)\nsummary.bins()\n\nvisualize\nvisualize two plots on a single screen. The plot at the top is a histogram representing the frequency of the level. The plot at the bottom is a bar chart representing the frequency of the level.\nplot.bins()\n\noptimal binning\ncategorizes a numeric characteristic into bins for ulterior usage in scoring modeling\nbinning_by()\n\nsummaries\nsummary metrics to evaluate the performance of binomial classification model\nsummary.optimal_bins()\n\nvisualize\ngenerates plots for understand distribution, bad rate, and weight of evidence after running binning_by()\nplot.optimal_bins()\n\nevaluate\ncalculates metrics to evaluate the performance of binned variable for binomial classification model\nperformance_bin()\n\nsummaries\nsummary metrics to evaluate the performance of binomial classification model after performance_bin()\nsummary.performance_bin()\n\nvisualize\nIt generates plots for understand frequency, WoE by bins using performance_bin after running binning_by()\nplot.performance_bin()\n\nvisualize\nextract bins from “bins” and “optimal_bins” objects\nextract.bins()\n\nDiagnose Binned Variable\nTypes\nDescriptions\nFunctions\nSupport DBI\ndiagnosis\nperforms diagnose performance that calculates metrics to evaluate the performance of binned variable for binomial classification model.\nperformance_bin()\n\nsummaries\nsummary method for “performance_bin”. summary metrics to evaluate the performance of binomial classification model.\nsummary.performance_bin()\n\nvisualize\nvisualize for understand frequency, WoE by bins using performance_bin and something else.\nplot.performance_bin()\n\nTransformation\nTypes\nDescriptions\nFunctions\nSupport DBI\ntransformation\nperforms variable transformation for standardization and resolving skewness of numerical variables.\ntransform()\n\nsummaries\ncompares the distribution of data before and after data transformation\nsummary.transform()\n\nvisualize\nvisualize two kinds of plot by attribute of ‘transform’ class. The transformation of a numerical variable is a density plot.\nplot.transform()\n\nReporting\nTypes\nDescriptions\nFunctions\nSupport DBI\nreporting the information of transformation into pdf\nreporting the information of transformation.\ntransformation_report()\n\nreporting the information of transformation into html\nreporting the information of transformation.\ntransformation_report()\n\nreporting the information of transformation into pdf\ndynamic reporting the information of transformation.\ntransformation_web_report()\n\nreporting the information of transformation into html\npaged reporting the information of transformation.\ntransformation_paged_report()\n\nMiscellaneous\nStatistics\nTypes\nDescriptions\nFunctions\nSupport DBI\nstatistics\ncalculate the entropy.\nentropy()\n\nstatistics\ncalculate the skewness of the data.\nskewness()\n\nstatistics\ncalculate the kurtosis of the data.\nkurtosis()\n\nstatistics\ncalculate the Jensen-Shannon divergence between two probability distributions.\njsd()\n\nstatistics\ncalculate the Kullback-Leibler divergence between two probability distributions.\nkld()\n\nstatistics\nfinding percentile of numerical variable.\nget_percentile()\n\nstatistics\ntransform a numeric vector using several methods like “log”, “sqrt”, “log+1”, “log+a”, “1/x”, “x^2”, “x^3”, “Box-Cox”, “Yeo-Johnson”\nget_transform()\n\nProgramming\nTypes\nDescriptions\nFunctions\nSupport DBI\nprogramming\nextracts variable information having a certain class from an object inheriting data.frame.\nfind_class()\n\nprogramming\ngets class of variables in data.frame or tbl_df.\nget_class()\n\nprogramming\nretrieves the column information of the DBMS table through the tbl_bdi object of dplyr.\nget_column_info()\n\nprogramming\nfinding Users Machine’s OS.\nget_os()\n\nprogramming\nimport Google Fonts.\nimport_google_font()\n\n\n\n\n",
      "last_modified": "2021-11-28T14:59:28+09:00"
    },
    {
      "path": "transformation.html",
      "title": "Data Transformation",
      "description": "Introduce dlookr package for derive new variables or perform variable transformations.\n",
      "author": [
        {
          "name": "Choonghyun Ryu",
          "url": "https://dataholic.netlify.app/"
        }
      ],
      "date": "2021-11-28",
      "contents": "\n\nContents\nPreface\ndatasets\nData Transformation\nImputation of missing values\nimputes the missing value with imputate_na()\nCollaboration with dplyr\n\nImputation of outliers\nimputes thr outliers with imputate_outlier()\nCollaboration with dplyr\n\nStandardization and Resolving Skewness\nIntroduction to the use of transform()\nStandardization with transform()\nResolving Skewness data with transform()\n\nBinning\nBinning of individual variables using binning()\nOptimal Binning with binning_by()\n\nAutomated report\nCreate a dynamic report using transformation_web_report()\nCreate a static report using transformation_paged_report()\n\n\n\n\n\nPreface\nAfter you have acquired the data, you should do the following:\nDiagnose data quality.\nIf there is a problem with data quality,\nThe data must be corrected or re-acquired.\n\nExplore data to understand the data and find scenarios for performing the analysis.\nDerive new variables or perform variable transformations.\nThe dlookr package makes these steps fast and easy:\nPerforms an data diagnosis or automatically generates a data diagnosis report.\nDiscover data in a variety of ways, and automatically generate EDA(exploratory data analysis) report.\nImpute missing values and outliers, resolve skewed data, and categorize continuous variables into categorical variables. And generates an automated report to support it.\nThis document introduces data transformation methods provided by the dlookr package. You will learn how to transform of tbl_df data that inherits from data.frame and data.frame with functions provided by dlookr.\ndlookr increases synergy with dplyr. Particularly in data transformation and data wrangle, it increases the efficiency of the tidyverse package group.\ndatasets\nTo illustrate the basic use of data transformation in the dlookr package, I use a Carseats dataset. Carseats in the ISLR package is simulation dataset that sells children’s car seats at 400 stores. This data is a data.frame created for the purpose of predicting sales volume.\n\n\nlibrary(ISLR)\nstr(Carseats)\n\n\n'data.frame':   400 obs. of  11 variables:\n $ Sales      : num  9.5 11.22 10.06 7.4 4.15 ...\n $ CompPrice  : num  138 111 113 117 141 124 115 136 132 132 ...\n $ Income     : num  73 48 35 100 64 113 105 81 110 113 ...\n $ Advertising: num  11 16 10 4 3 13 0 15 0 0 ...\n $ Population : num  276 260 269 466 340 501 45 425 108 131 ...\n $ Price      : num  120 83 80 97 128 72 108 120 124 124 ...\n $ ShelveLoc  : Factor w/ 3 levels \"Bad\",\"Good\",\"Medium\": 1 2 3 3 1 1 3 2 3 3 ...\n $ Age        : num  42 65 59 55 38 78 71 67 76 76 ...\n $ Education  : num  17 10 12 14 13 16 15 10 10 17 ...\n $ Urban      : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 1 2 2 1 1 ...\n $ US         : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 1 2 1 2 ...\n\nThe contents of individual variables are as follows. (Refer to ISLR::Carseats Man page)\nSales\nUnit sales (in thousands) at each location\n\nCompPrice\nPrice charged by competitor at each location\n\nIncome\nCommunity income level (in thousands of dollars)\n\nAdvertising\nLocal advertising budget for company at each location (in thousands of dollars)\n\nPopulation\nPopulation size in region (in thousands)\n\nPrice\nPrice company charges for car seats at each site\n\nShelveLoc\nA factor with levels Bad, Good and Medium indicating the quality of the shelving location for the car seats at each site\n\nAge\nAverage age of the local population\n\nEducation\nEducation level at each location\n\nUrban\nA factor with levels No and Yes to indicate whether the store is in an urban or rural location\n\nUS\nA factor with levels No and Yes to indicate whether the store is in the US or not\n\nWhen data analysis is performed, data containing missing values is often encountered. However, Carseats is complete data without missing. Therefore, the missing values are generated as follows. And I created a data.frame object named carseats.\n\n\ncarseats <- ISLR::Carseats\n\nsuppressWarnings(RNGversion(\"3.5.0\"))\nset.seed(123)\ncarseats[sample(seq(NROW(carseats)), 20), \"Income\"] <- NA\n\nsuppressWarnings(RNGversion(\"3.5.0\"))\nset.seed(456)\ncarseats[sample(seq(NROW(carseats)), 10), \"Urban\"] <- NA\n\n\n\nData Transformation\ndlookr imputes missing values and outliers and resolves skewed data. It also provides the ability to bin continuous variables as categorical variables.\nHere is a list of the data conversion functions and functions provided by dlookr:\nfind_na() finds a variable that contains the missing values variable, and imputate_na() imputes the missing values.\nfind_outliers() finds a variable that contains the outliers, and imputate_outlier() imputes the outlier.\nsummary.imputation() and plot.imputation() provide information and visualization of the imputed variables.\nfind_skewness() finds the variables of the skewed data, and transform() performs the resolving of the skewed data.\ntransform() also performs standardization of numeric variables.\nsummary.transform() and plot.transform() provide information and visualization of transformed variables.\nbinning() and binning_by() convert binational data into categorical data.\nprint.bins() and summary.bins() show and summarize the binning results.\nplot.bins() and plot.optimal_bins() provide visualization of the binning result.\ntransformation_report() performs the data transform and reports the result.\nImputation of missing values\nimputes the missing value with imputate_na()\nimputate_na() imputes the missing value contained in the variable. The predictor with missing values support both numeric and categorical variables, and supports the following method.\npredictor is numerical variable\n“mean” : arithmetic mean\n“median” : median\n“mode” : mode\n“knn” : K-nearest neighbors\ntarget variable must be specified\n\n“rpart” : Recursive Partitioning and Regression Trees\ntarget variable must be specified\n\n“mice” : Multivariate Imputation by Chained Equations\ntarget variable must be specified\nrandom seed must be set\n\n\npredictor is categorical variable\n“mode” : mode\n“rpart” : Recursive Partitioning and Regression Trees\ntarget variable must be specified\n\n“mice” : Multivariate Imputation by Chained Equations\ntarget variable must be specified\nrandom seed must be set\n\n\nIn the following example, imputate_na() imputes the missing value of Income, a numeric variable of carseats, using the “rpart” method. summary() summarizes missing value imputation information, and plot() visualizes missing information.\n\n\nif (requireNamespace(\"rpart\", quietly = TRUE)) {\n  income <- imputate_na(carseats, Income, US, method = \"rpart\")\n\n  # result of imputation\n  income\n\n  # summary of imputation\n  summary(income)\n\n  # viz of imputation\n  plot(income)\n} else {\n  cat(\"If you want to use this feature, you need to install the rpart package.\\n\")\n}\n\n\n* Impute missing values based on Recursive Partitioning and Regression Trees\n - method : rpart\n\n* Information of Imputation (before vs after)\n           Original   Imputation\nn        380.000000 400.00000000\nna        20.000000   0.00000000\nmean      68.860526  69.05073137\nsd        28.091615  27.57381661\nse_mean    1.441069   1.37869083\nIQR       48.250000  46.00000000\nskewness   0.044906   0.02935732\nkurtosis  -1.089201  -1.03508622\np00       21.000000  21.00000000\np01       21.790000  21.99000000\np05       26.000000  26.00000000\np10       30.000000  30.90000000\np20       39.000000  40.00000000\np25       42.750000  44.00000000\np30       48.000000  51.58333333\np40       62.000000  63.00000000\np50       69.000000  69.00000000\np60       78.000000  77.40000000\np70       86.300000  84.30000000\np75       91.000000  90.00000000\np80       96.200000  96.00000000\np90      108.100000 106.10000000\np95      115.050000 115.00000000\np99      119.210000 119.01000000\np100     120.000000 120.00000000\n\n\nThe following imputes the categorical variable urban by the “mice” method.\n\n\nlibrary(mice)\n\nurban <- imputate_na(carseats, Urban, US, method = \"mice\")\n\n\n\n iter imp variable\n  1   1  Income  Urban\n  1   2  Income  Urban\n  1   3  Income  Urban\n  1   4  Income  Urban\n  1   5  Income  Urban\n  2   1  Income  Urban\n  2   2  Income  Urban\n  2   3  Income  Urban\n  2   4  Income  Urban\n  2   5  Income  Urban\n  3   1  Income  Urban\n  3   2  Income  Urban\n  3   3  Income  Urban\n  3   4  Income  Urban\n  3   5  Income  Urban\n  4   1  Income  Urban\n  4   2  Income  Urban\n  4   3  Income  Urban\n  4   4  Income  Urban\n  4   5  Income  Urban\n  5   1  Income  Urban\n  5   2  Income  Urban\n  5   3  Income  Urban\n  5   4  Income  Urban\n  5   5  Income  Urban\n\n\n# result of imputation\nurban\n\n\n  [1] Yes Yes Yes Yes Yes No  Yes Yes No  No  No  Yes Yes Yes Yes No \n [17] Yes Yes No  Yes Yes No  Yes Yes Yes No  No  Yes Yes Yes Yes Yes\n [33] Yes Yes Yes No  No  Yes Yes No  No  Yes Yes Yes Yes Yes No  Yes\n [49] Yes Yes Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes No  Yes Yes\n [65] No  No  Yes Yes Yes Yes Yes No  Yes No  No  No  Yes No  Yes Yes\n [81] Yes Yes Yes No  No  No  Yes No  Yes No  No  Yes Yes No  Yes Yes\n [97] No  Yes No  No  No  Yes No  Yes Yes Yes No  Yes Yes No  Yes Yes\n[113] Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes No  Yes No  Yes Yes\n[129] Yes No  Yes Yes Yes Yes Yes No  No  Yes Yes No  Yes Yes Yes Yes\n[145] No  Yes Yes No  No  Yes Yes No  No  No  No  Yes Yes No  No  No \n[161] No  No  Yes No  No  Yes Yes Yes Yes Yes Yes Yes Yes Yes No  Yes\n[177] No  Yes No  Yes Yes Yes Yes Yes No  Yes No  Yes Yes No  No  Yes\n[193] No  Yes Yes Yes Yes Yes Yes Yes No  Yes No  Yes Yes Yes Yes No \n[209] Yes No  No  Yes Yes Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes\n[225] No  Yes Yes Yes No  No  No  No  Yes No  No  Yes Yes Yes Yes Yes\n[241] Yes Yes No  Yes Yes No  Yes Yes Yes Yes Yes Yes Yes No  Yes Yes\n[257] Yes Yes No  No  Yes Yes Yes Yes Yes Yes No  No  Yes Yes Yes Yes\n[273] Yes Yes Yes Yes Yes Yes No  Yes Yes No  Yes No  No  Yes No  Yes\n[289] No  Yes No  No  Yes Yes Yes No  Yes Yes Yes No  Yes Yes Yes Yes\n[305] Yes Yes Yes Yes Yes Yes Yes Yes No  Yes Yes Yes Yes No  No  No \n[321] Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No  Yes Yes Yes Yes Yes\n[337] Yes Yes Yes Yes Yes No  No  Yes No  Yes No  No  Yes No  No  No \n[353] Yes No  Yes Yes Yes Yes Yes Yes No  No  Yes Yes Yes No  No  Yes\n[369] No  Yes Yes Yes No  Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes\n[385] Yes Yes Yes No  Yes Yes Yes Yes Yes No  Yes Yes No  Yes Yes Yes\nattr(,\"var_type\")\n[1] categorical\nattr(,\"method\")\n[1] mice\nattr(,\"na_pos\")\n [1]  33  36  84  94 113 132 151 292 313 339\nattr(,\"seed\")\n[1] 24283\nattr(,\"type\")\n[1] missing values\nattr(,\"message\")\n[1] complete imputation\nattr(,\"success\")\n[1] TRUE\nLevels: No Yes\n\n\n# summary of imputation\nsummary(urban)\n\n\n* Impute missing values based on Multivariate Imputation by Chained Equations\n - method : mice\n - random seed : 24283\n\n* Information of Imputation (before vs after)\n     original imputation original_percent imputation_percent\nNo        115        120            28.75                 30\nYes       275        280            68.75                 70\n<NA>       10          0             2.50                  0\n\n\n# viz of imputation\nplot(urban)\n\n\n\n\nCollaboration with dplyr\nThe following example imputes the missing value of the Income variable, and then calculates the arithmetic mean for each level of US. In this case, dplyr is used, and it is easily interpreted logically using pipes.\n\n\n# The mean before and after the imputation of the Income variable\ncarseats %>%\n  mutate(Income_imp = imputate_na(carseats, Income, US, method = \"knn\")) %>%\n  group_by(US) %>%\n  summarise(orig = mean(Income, na.rm = TRUE),\n            imputation = mean(Income_imp))\n\n\n# A tibble: 2 x 3\n  US     orig imputation\n  <fct> <dbl>      <dbl>\n1 No     65.8       66.1\n2 Yes    70.4       70.5\n\nImputation of outliers\nimputes thr outliers with imputate_outlier()\nimputate_outlier() imputes the outliers value. The predictor with outliers supports only numeric variables and supports the following methods.\npredictor is numerical variable\n“mean” : arithmetic mean\n“median” : median\n“mode” : mode\n“capping” : Impute the upper outliers with 95 percentile, and Impute the bottom outliers with 5 percentile.\n\nimputate_outlier() imputes the outliers with the numeric variable Price as the “capping” method, as follows. summary() summarizes outliers imputation information, and plot() visualizes imputation information.\n\n\nprice <- imputate_outlier(carseats, Price, method = \"capping\")\n\n# result of imputation\nprice\n\n\n  [1] 120.00  83.00  80.00  97.00 128.00  72.00 108.00 120.00 124.00\n [10] 124.00 100.00  94.00 136.00  86.00 118.00 144.00 110.00 131.00\n [19]  68.00 121.00 131.00 109.00 138.00 109.00 113.00  82.00 131.00\n [28] 107.00  97.00 102.00  89.00 131.00 137.00 128.00 128.00  96.00\n [37] 100.00 110.00 102.00 138.00 126.00 124.00  77.00 134.00  95.00\n [46] 135.00  70.00 108.00  98.00 149.00 108.00 108.00 129.00 119.00\n [55] 144.00 154.00  84.00 117.00 103.00 114.00 123.00 107.00 133.00\n [64] 101.00 104.00 128.00  91.00 115.00 134.00  99.00  99.00 150.00\n [73] 116.00 104.00 136.00  92.00  70.00  89.00 145.00  90.00  79.00\n [82] 128.00 139.00  94.00 121.00 112.00 134.00 126.00 111.00 119.00\n [91] 103.00 107.00 125.00 104.00  84.00 148.00 132.00 129.00 127.00\n[100] 107.00 106.00 118.00  97.00  96.00 138.00  97.00 139.00 108.00\n[109] 103.00  90.00 116.00 151.00 125.00 127.00 106.00 129.00 128.00\n[118] 119.00  99.00 128.00 131.00  87.00 108.00 155.00 120.00  77.00\n[127] 133.00 116.00 126.00 147.00  77.00  94.00 136.00  97.00 131.00\n[136] 120.00 120.00 118.00 109.00  94.00 129.00 131.00 104.00 159.00\n[145] 123.00 117.00 131.00 119.00  97.00  87.00 114.00 103.00 128.00\n[154] 150.00 110.00  69.00 157.00  90.00 112.00  70.00 111.00 160.00\n[163] 149.00 106.00 141.00 155.05 137.00  93.00 117.00  77.00 118.00\n[172]  55.00 110.00 128.00 155.05 122.00 154.00  94.00  81.00 116.00\n[181] 149.00  91.00 140.00 102.00  97.00 107.00  86.00  96.00  90.00\n[190] 104.00 101.00 173.00  93.00  96.00 128.00 112.00 133.00 138.00\n[199] 128.00 126.00 146.00 134.00 130.00 157.00 124.00 132.00 160.00\n[208]  97.00  64.00  90.00 123.00 120.00 105.00 139.00 107.00 144.00\n[217] 144.00 111.00 120.00 116.00 124.00 107.00 145.00 125.00 141.00\n[226]  82.00 122.00 101.00 163.00  72.00 114.00 122.00 105.00 120.00\n[235] 129.00 132.00 108.00 135.00 133.00 118.00 121.00  94.00 135.00\n[244] 110.00 100.00  88.00  90.00 151.00 101.00 117.00 156.00 132.00\n[253] 117.00 122.00 129.00  81.00 144.00 112.00  81.00 100.00 101.00\n[262] 118.00 132.00 115.00 159.00 129.00 112.00 112.00 105.00 166.00\n[271]  89.00 110.00  63.00  86.00 119.00 132.00 130.00 125.00 151.00\n[280] 158.00 145.00 105.00 154.00 117.00  96.00 131.00 113.00  72.00\n[289]  97.00 156.00 103.00  89.00  74.00  89.00  99.00 137.00 123.00\n[298] 104.00 130.00  96.00  99.00  87.00 110.00  99.00 134.00 132.00\n[307] 133.00 120.00 126.00  80.00 166.00 132.00 135.00  54.00 129.00\n[316] 171.00  72.00 136.00 130.00 129.00 152.00  98.00 139.00 103.00\n[325] 150.00 104.00 122.00 104.00 111.00  89.00 112.00 134.00 104.00\n[334] 147.00  83.00 110.00 143.00 102.00 101.00 126.00  91.00  93.00\n[343] 118.00 121.00 126.00 149.00 125.00 112.00 107.00  96.00  91.00\n[352] 105.00 122.00  92.00 145.00 146.00 164.00  72.00 118.00 130.00\n[361] 114.00 104.00 110.00 108.00 131.00 162.00 134.00  77.00  79.00\n[370] 122.00 119.00 126.00  98.00 116.00 118.00 124.00  92.00 125.00\n[379] 119.00 107.00  89.00 151.00 121.00  68.00 112.00 132.00 160.00\n[388] 115.00  78.00 107.00 111.00 124.00 130.00 120.00 139.00 128.00\n[397] 120.00 159.00  95.00 120.00\nattr(,\"method\")\n[1] \"capping\"\nattr(,\"var_type\")\n[1] \"numerical\"\nattr(,\"outlier_pos\")\n[1]  43 126 166 175 368\nattr(,\"outliers\")\n[1]  24  49 191 185  53\nattr(,\"type\")\n[1] \"outliers\"\nattr(,\"message\")\n[1] \"complete imputation\"\nattr(,\"success\")\n[1] TRUE\nattr(,\"class\")\n[1] \"imputation\" \"numeric\"   \n\n\n# summary of imputation\nsummary(price)\n\n\nImpute outliers with capping\n\n* Information of Imputation (before vs after)\n            Original  Imputation\nn        400.0000000 400.0000000\nna         0.0000000   0.0000000\nmean     115.7950000 115.8927500\nsd        23.6766644  22.6109187\nse_mean    1.1838332   1.1305459\nIQR       31.0000000  31.0000000\nskewness  -0.1252862  -0.0461621\nkurtosis   0.4518850  -0.3030578\np00       24.0000000  54.0000000\np01       54.9900000  67.9600000\np05       77.0000000  77.0000000\np10       87.0000000  87.0000000\np20       96.8000000  96.8000000\np25      100.0000000 100.0000000\np30      104.0000000 104.0000000\np40      110.0000000 110.0000000\np50      117.0000000 117.0000000\np60      122.0000000 122.0000000\np70      128.3000000 128.3000000\np75      131.0000000 131.0000000\np80      134.0000000 134.0000000\np90      146.0000000 146.0000000\np95      155.0500000 155.0025000\np99      166.0500000 164.0200000\np100     191.0000000 173.0000000\n\n\n# viz of imputation\nplot(price)\n\n\n\n\nCollaboration with dplyr\nThe following example imputes the outliers of the Price variable, and then calculates the arithmetic mean for each level of US. In this case, dplyr is used, and it is easily interpreted logically using pipes.\n\n\n# The mean before and after the imputation of the Price variable\ncarseats %>%\n  mutate(Price_imp = imputate_outlier(carseats, Price, method = \"capping\")) %>%\n  group_by(US) %>%\n  summarise(orig = mean(Price, na.rm = TRUE),\n    imputation = mean(Price_imp, na.rm = TRUE))\n\n\n# A tibble: 2 x 3\n  US     orig imputation\n  <fct> <dbl>      <dbl>\n1 No     114.       114.\n2 Yes    117.       117.\n\nStandardization and Resolving Skewness\nIntroduction to the use of transform()\ntransform() performs data transformation. Only numeric variables are supported, and the following methods are provided.\nStandardization\n“zscore” : z-score transformation. (x - mu) / sigma\n“minmax” : minmax transformation. (x - min) / (max - min)\n\nResolving Skewness\n“log” : log transformation. log(x)\n“log+1” : log transformation. log(x + 1). Used for values that contain 0.\n“sqrt” : square root transformation.\n“1/x” : 1 / x transformation\n“x^2” : x square transformation\n“x^3” : x^3 square transformation\n\nStandardization with transform()\nUse the methods “zscore” and “minmax” to perform standardization.\n\n\ncarseats %>% \n  mutate(Income_minmax = transform(carseats$Income, method = \"minmax\"),\n    Sales_minmax = transform(carseats$Sales, method = \"minmax\")) %>% \n  select(Income_minmax, Sales_minmax) %>% \n  boxplot()\n\n\n\n\nResolving Skewness data with transform()\nfind_skewness() searches for variables with skewed data. This function finds data skewed by search conditions and calculates skewness.\n\n\n# find index of skewed variables\nfind_skewness(carseats)\n\n\n[1] 4\n\n\n# find names of skewed variables\nfind_skewness(carseats, index = FALSE)\n\n\n[1] \"Advertising\"\n\n\n# compute the skewness\nfind_skewness(carseats, value = TRUE)\n\n\n      Sales   CompPrice      Income Advertising  Population \n      0.185      -0.043       0.045       0.637      -0.051 \n      Price         Age   Education \n     -0.125      -0.077       0.044 \n\n\n# compute the skewness & filtering with threshold\nfind_skewness(carseats, value = TRUE, thres = 0.1)\n\n\n      Sales Advertising       Price \n      0.185       0.637      -0.125 \n\nThe skewness of Advertising is 0.637. This means that the distribution of data is somewhat inclined to the left. So, for normal distribution, use transform() to convert to “log” method as follows. summary() summarizes transformation information, and plot() visualizes transformation information.\n\n\nAdvertising_log = transform(carseats$Advertising, method = \"log\")\n\n# result of transformation\nhead(Advertising_log)\n\n\n[1] 2.397895 2.772589 2.302585 1.386294 1.098612 2.564949\n\n# summary of transformation\nsummary(Advertising_log)\n\n\n* Resolving Skewness with log\n\n* Information of Transformation (before vs after)\n            Original Transformation\nn        400.0000000    400.0000000\nna         0.0000000      0.0000000\nmean       6.6350000           -Inf\nsd         6.6503642            NaN\nse_mean    0.3325182            NaN\nIQR       12.0000000            Inf\nskewness   0.6395858            NaN\nkurtosis  -0.5451178            NaN\np00        0.0000000           -Inf\np01        0.0000000           -Inf\np05        0.0000000           -Inf\np10        0.0000000           -Inf\np20        0.0000000           -Inf\np25        0.0000000           -Inf\np30        0.0000000           -Inf\np40        2.0000000      0.6931472\np50        5.0000000      1.6094379\np60        8.4000000      2.1265548\np70       11.0000000      2.3978953\np75       12.0000000      2.4849066\np80       13.0000000      2.5649494\np90       16.0000000      2.7725887\np95       19.0000000      2.9444390\np99       23.0100000      3.1359198\np100      29.0000000      3.3672958\n\n# viz of transformation\nplot(Advertising_log)\n\n\n\n\nIt seems that the raw data contains 0, as there is a -Inf in the log converted value. So this time, convert it to “log+1”.\n\n\nAdvertising_log <- transform(carseats$Advertising, method = \"log+1\")\n\n# result of transformation\nhead(Advertising_log)\n\n\n[1] 2.484907 2.833213 2.397895 1.609438 1.386294 2.639057\n\n# summary of transformation\nsummary(Advertising_log)\n\n\n* Resolving Skewness with log+1\n\n* Information of Transformation (before vs after)\n            Original Transformation\nn        400.0000000   400.00000000\nna         0.0000000     0.00000000\nmean       6.6350000     1.46247709\nsd         6.6503642     1.19436323\nse_mean    0.3325182     0.05971816\nIQR       12.0000000     2.56494936\nskewness   0.6395858    -0.19852549\nkurtosis  -0.5451178    -1.66342876\np00        0.0000000     0.00000000\np01        0.0000000     0.00000000\np05        0.0000000     0.00000000\np10        0.0000000     0.00000000\np20        0.0000000     0.00000000\np25        0.0000000     0.00000000\np30        0.0000000     0.00000000\np40        2.0000000     1.09861229\np50        5.0000000     1.79175947\np60        8.4000000     2.23936878\np70       11.0000000     2.48490665\np75       12.0000000     2.56494936\np80       13.0000000     2.63905733\np90       16.0000000     2.83321334\np95       19.0000000     2.99573227\np99       23.0100000     3.17846205\np100      29.0000000     3.40119738\n\n# viz of transformation\n# plot(Advertising_log)\n\n\n\nBinning\nBinning of individual variables using binning()\nbinning() transforms a numeric variable into a categorical variable by binning it. The following types of binning are supported.\n“quantile” : categorize using quantile to include the same frequencies\n“equal” : categorize to have equal length segments\n“pretty” : categorized into moderately good segments\n“kmeans” : categorization using K-means clustering\n“bclust” : categorization using bagged clustering technique\nHere are some examples of how to bin Income using binning().:\n\n\n# Binning the carat variable. default type argument is \"quantile\"\nbin <- binning(carseats$Income)\n# Print bins class object\nbin\n\n\nbinned type: quantile\nnumber of bins: 10\nx\n        [21,30]         (30,39]         (39,48]         (48,62] \n             40              37              38              40 \n        (62,69]         (69,78]   (78,86.56667] (86.56667,96.6] \n             42              33              36              38 \n(96.6,108.6333]  (108.6333,120]            <NA> \n             38              38              20 \n\n# Summarize bins class object\nsummary(bin)\n\n\n            levels freq   rate\n1          [21,30]   40 0.1000\n2          (30,39]   37 0.0925\n3          (39,48]   38 0.0950\n4          (48,62]   40 0.1000\n5          (62,69]   42 0.1050\n6          (69,78]   33 0.0825\n7    (78,86.56667]   36 0.0900\n8  (86.56667,96.6]   38 0.0950\n9  (96.6,108.6333]   38 0.0950\n10  (108.6333,120]   38 0.0950\n11            <NA>   20 0.0500\n\n# Plot bins class object\nplot(bin)\n\n\n\n# Using labels argument\nbin <- binning(carseats$Income, nbins = 4,\n              labels = c(\"LQ1\", \"UQ1\", \"LQ3\", \"UQ3\"))\nbin\n\n\nbinned type: quantile\nnumber of bins: 4\nx\n LQ1  UQ1  LQ3  UQ3 <NA> \n  95  102   89   94   20 \n\n# Using another type argument\nbinning(carseats$Income, nbins = 5, type = \"equal\")\n\n\nbinned type: equal\nnumber of bins: 5\nx\n   [21,40.8]  (40.8,60.6]  (60.6,80.4] (80.4,100.2]  (100.2,120] \n          81           65           94           80           60 \n        <NA> \n          20 \n\nbinning(carseats$Income, nbins = 5, type = \"pretty\")\n\n\nbinned type: pretty\nnumber of bins: 5\nx\n  [20,40]   (40,60]   (60,80]  (80,100] (100,120]      <NA> \n       81        65        94        80        60        20 \n\n\nif (requireNamespace(\"classInt\", quietly = TRUE)) {\n  binning(carseats$Income, nbins = 5, type = \"kmeans\")\n  binning(carseats$Income, nbins = 5, type = \"bclust\")\n} else {\n  cat(\"If you want to use this feature, you need to install the classInt package.\\n\")\n}\n\n\nbinned type: bclust\nnumber of bins: 5\nx\n  [21,30.5]   (30.5,49]   (49,75.5] (75.5,97.5]  (97.5,120] \n         40          75         104          86          75 \n       <NA> \n         20 \n\n\n# Extract the binned results\nextract(bin)\n\n\n  [1] LQ3  UQ1  LQ1  UQ3  UQ1  UQ3  UQ3  LQ3  UQ3  UQ3  LQ3  UQ3  LQ1 \n [14] LQ1  UQ3  UQ3  <NA> <NA> UQ3  LQ3  LQ3  LQ1  UQ1  LQ1  UQ3  LQ1 \n [27] UQ3  UQ3  LQ3  UQ3  UQ3  UQ1  LQ1  LQ1  UQ1  LQ3  LQ3  LQ1  LQ3 \n [40] <NA> UQ3  UQ1  UQ1  LQ1  LQ3  UQ1  LQ3  UQ3  UQ1  UQ3  LQ1  LQ3 \n [53] LQ1  UQ1  UQ3  LQ3  LQ3  LQ3  UQ3  LQ3  UQ3  LQ1  UQ1  LQ3  UQ1 \n [66] LQ1  UQ3  UQ1  UQ1  UQ1  LQ3  UQ1  UQ1  LQ3  UQ1  UQ3  LQ3  LQ3 \n [79] UQ1  UQ1  UQ3  LQ3  LQ3  LQ1  LQ1  UQ3  LQ3  UQ1  LQ1  UQ1  LQ1 \n [92] UQ1  UQ3  LQ1  <NA> LQ1  LQ1  LQ3  LQ3  UQ1  UQ1  UQ3  LQ1  LQ3 \n[105] UQ3  UQ3  LQ1  UQ3  LQ3  UQ1  UQ1  UQ3  UQ3  LQ1  LQ3  <NA> LQ3 \n[118] UQ1  LQ3  UQ3  UQ3  LQ3  UQ3  UQ3  UQ3  <NA> UQ1  UQ1  UQ3  UQ3 \n[131] LQ3  UQ1  LQ3  UQ3  LQ1  UQ3  LQ3  LQ1  UQ3  UQ1  UQ1  LQ1  LQ3 \n[144] LQ3  UQ1  UQ1  LQ3  UQ1  UQ3  UQ3  LQ3  UQ1  LQ3  LQ1  UQ1  LQ3 \n[157] LQ1  UQ1  LQ3  UQ1  LQ1  LQ1  <NA> UQ1  UQ1  UQ1  UQ1  LQ3  LQ3 \n[170] LQ1  LQ1  UQ3  UQ3  LQ3  LQ1  LQ3  <NA> LQ3  <NA> LQ1  UQ3  LQ3 \n[183] UQ1  LQ3  LQ1  UQ3  UQ1  LQ1  LQ1  UQ3  LQ1  LQ1  LQ1  LQ3  UQ3 \n[196] UQ3  LQ1  UQ1  LQ3  LQ3  UQ3  LQ3  LQ3  LQ3  LQ3  LQ1  UQ1  UQ3 \n[209] <NA> LQ1  LQ1  UQ3  UQ1  LQ3  UQ3  LQ3  <NA> UQ1  UQ1  LQ3  UQ3 \n[222] <NA> UQ3  UQ1  LQ3  LQ1  LQ1  UQ1  LQ3  UQ3  UQ1  UQ1  LQ3  LQ3 \n[235] UQ1  LQ1  LQ1  LQ1  LQ1  UQ3  LQ3  UQ1  UQ1  LQ1  LQ1  UQ1  UQ1 \n[248] UQ3  UQ1  UQ1  UQ3  UQ3  UQ3  LQ1  UQ3  LQ3  LQ1  UQ1  LQ1  LQ1 \n[261] UQ3  LQ1  <NA> LQ1  LQ1  LQ1  UQ3  LQ3  UQ1  UQ1  LQ1  UQ1  LQ1 \n[274] UQ3  UQ3  UQ3  UQ1  UQ1  UQ3  UQ1  LQ3  UQ1  UQ3  UQ3  UQ1  LQ1 \n[287] UQ3  UQ1  LQ1  LQ3  UQ3  LQ3  UQ1  LQ3  LQ3  LQ1  UQ1  LQ3  UQ1 \n[300] LQ1  LQ3  UQ3  LQ3  UQ1  UQ3  LQ1  LQ1  UQ3  LQ3  UQ3  UQ1  UQ1 \n[313] UQ3  LQ3  <NA> LQ1  LQ1  LQ1  LQ3  UQ1  LQ3  LQ1  UQ1  UQ3  UQ1 \n[326] UQ1  LQ1  LQ1  UQ1  UQ1  UQ1  UQ1  LQ1  UQ1  UQ3  LQ3  LQ1  LQ1 \n[339] LQ1  UQ1  LQ1  UQ3  UQ3  LQ1  LQ3  UQ1  <NA> LQ1  UQ3  LQ1  <NA>\n[352] UQ3  UQ3  UQ1  LQ1  UQ3  UQ3  LQ3  UQ3  UQ1  LQ3  LQ1  UQ1  <NA>\n[365] LQ1  LQ1  UQ1  UQ3  LQ1  UQ3  LQ1  LQ3  <NA> <NA> UQ1  UQ1  UQ1 \n[378] UQ1  LQ3  UQ3  UQ1  UQ1  LQ1  UQ3  LQ1  LQ3  UQ3  LQ3  LQ3  LQ1 \n[391] LQ3  UQ1  LQ1  UQ1  UQ1  UQ3  <NA> LQ1  LQ3  LQ1 \nLevels: LQ1 < UQ1 < LQ3 < UQ3\n\n\n# -------------------------\n# Using pipes & dplyr\n# -------------------------\nlibrary(dplyr)\n\ncarseats %>%\n mutate(Income_bin = binning(carseats$Income) %>% \n                     extract()) %>%\n group_by(ShelveLoc, Income_bin) %>%\n summarise(freq = n()) %>%\n arrange(desc(freq)) %>%\n head(10)\n\n\n# A tibble: 10 x 3\n# Groups:   ShelveLoc [1]\n  ShelveLoc Income_bin  freq\n  <fct>     <ord>      <int>\n1 Medium    [21,30]       25\n2 Medium    (62,69]       24\n3 Medium    (48,62]       23\n4 Medium    (39,48]       21\n# … with 6 more rows\n\nOptimal Binning with binning_by()\nbinning_by() transforms a numeric variable into a categorical variable by optimal binning. This method is often used when developing a scorecard model.\nThe following binning_by() example optimally binning Advertising considering the target variable US with a binary class.\n\n\nlibrary(dplyr)\n\n# optimal binning using character\nbin <- binning_by(carseats, \"US\", \"Advertising\")\n\n# optimal binning using name\nbin <- binning_by(carseats, US, Advertising)\nbin\n\n\nbinned type: optimal\nnumber of bins: 3\nx\n[-1,0]  (0,6] (6,29] \n   144     69    187 \n\n\n# summary optimal_bins class\nsummary(bin)\n\n\n── Binning Table ──────────────────────── Several Metrics ── \n     Bin CntRec CntPos CntNeg RatePos RateNeg    Odds      WoE\n1 [-1,0]    144     19    125 0.07364 0.88028  0.1520 -2.48101\n2  (0,6]     69     54     15 0.20930 0.10563  3.6000  0.68380\n3 (6,29]    187    185      2 0.71705 0.01408 92.5000  3.93008\n4  Total    400    258    142 1.00000 1.00000  1.8169       NA\n       IV     JSD     AUC\n1 2.00128 0.20093 0.03241\n2 0.07089 0.00869 0.01883\n3 2.76272 0.21861 0.00903\n4 4.83489 0.42823 0.06028\n\n── General Metrics ───────────────────────────────────────── \n• Gini index                       :  -0.87944\n• IV (Jeffrey)                     :  4.83489\n• JS (Jensen-Shannon) Divergence   :  0.42823\n• Kolmogorov-Smirnov Statistics    :  0.80664\n• HHI (Herfindahl-Hirschman Index) :  0.37791\n• HHI (normalized)                 :  0.06687\n• Cramer's V                       :  0.81863 \n\n── Significance Tests ──────────────────── Chisquare Test ── \n   Bin A  Bin B statistics      p_value\n1 [-1,0]  (0,6]   87.67064 7.731349e-21\n2  (0,6] (6,29]   34.73349 3.780706e-09\n\n\n# performance table\nattr(bin, \"performance\")\n\n\n     Bin CntRec CntPos CntNeg CntCumPos CntCumNeg RatePos RateNeg\n1 [-1,0]    144     19    125        19       125 0.07364 0.88028\n2  (0,6]     69     54     15        73       140 0.20930 0.10563\n3 (6,29]    187    185      2       258       142 0.71705 0.01408\n4  Total    400    258    142        NA        NA 1.00000 1.00000\n  RateCumPos RateCumNeg    Odds   LnOdds      WoE      IV     JSD\n1    0.07364    0.88028  0.1520 -1.88387 -2.48101 2.00128 0.20093\n2    0.28295    0.98592  3.6000  1.28093  0.68380 0.07089 0.00869\n3    1.00000    1.00000 92.5000  4.52721  3.93008 2.76272 0.21861\n4         NA         NA  1.8169  0.59713       NA 4.83489 0.42823\n      AUC\n1 0.03241\n2 0.01883\n3 0.00903\n4 0.06028\n\n\n# visualize optimal_bins class\nplot(bin)\n\n\n\n\n# extract binned results\nextract(bin) %>% \n  head(20)\n\n\n [1] (6,29] (6,29] (6,29] (0,6]  (0,6]  (6,29] [-1,0] (6,29] [-1,0]\n[10] [-1,0] (6,29] (0,6]  (0,6]  (6,29] (6,29] (0,6]  [-1,0] (6,29]\n[19] [-1,0] (6,29]\nLevels: [-1,0] < (0,6] < (6,29]\n\nAutomated report\ndlookr provides two automated data transformation reports:\nWeb page-based dynamic reports can perform in-depth analysis through visualization and statistical tables.\nStatic reports generated as pdf files or html files can be archived as output of data analysis.\nCreate a dynamic report using transformation_web_report()\ntransformation_web_report() create dynamic report for object inherited from data.frame(tbl_df, tbl, etc) or data.frame.\nContents of dynamic web report\nThe contents of the report are as follows.:\nOverview\nData Structures\nData Types\nJob Informations\n\nImputation\nMissing Values\nOutliers\n\nResolving Skewness\nBinning\nOptimal Binning\nSome arguments for dynamic web report\ntransformation_web_report() generates various reports with the following arguments.\ntarget\ntarget variable\n\noutput_file\nname of generated file.\n\noutput_dir\nname of directory to generate report file.\n\ntitle\ntitle of report.\n\nsubtitle\nsubtitle of report.\n\nauthor\nauthor of report.\n\ntitle_color\ncolor of title.\n\nlogo_img\nname of logo image file on top left.\n\ncreate_date\nThe date on which the report is generated.\n\ntheme\nname of theme for report. support “orange” and “blue”.\n\nsample_percent\nSample percent of data for performing data transformation.\n\nThe following script creates a data transformation report for the tbl_df class object, heartfailure.\n\n\nheartfailure %>%\n  transformation_web_report(target = \"death_event\", subtitle = \"heartfailure\",\n                            output_dir = \"./\", output_file = \"transformation.html\", \n                            theme = \"blue\")\n\n\n\nScreenshot of dynamic report\nThe dynamic contents of the report is shown in the following figure.:\n\n\n\n(#fig:trans_web_title)The part of the report\n\n\n\nCreate a static report using transformation_paged_report()\ntransformation_paged_report() create static report for object inherited from data.frame(tbl_df, tbl, etc) or data.frame.\nContents of static paged report\nThe contents of the report are as follows.:\nOverview\nData Structures\nJob Informations\n\nImputation\nMissing Values\nOutliers\n\nResolving Skewness\nBinning\nOptimal Binning\nSome arguments for static paged report\ntransformation_paged_report() generates various reports with the following arguments.\ntarget\ntarget variable\n\noutput_format\nreport output type. Choose either “pdf” and “html”.\n\noutput_file\nname of generated file.\n\noutput_dir\nname of directory to generate report file.\n\ntitle\ntitle of report.\n\nsubtitle\nsubtitle of report.\n\nabstract_title\nabstract of report\n\nauthor\nauthor of report.\n\ntitle_color\ncolor of title.\n\nsubtitle_color\ncolor of subtitle.\n\nlogo_img\nname of logo image file on top left.\n\ncover_img\nname of cover image file on center.\n\ncreate_date\nThe date on which the report is generated.\n\ntheme\nname of theme for report. support “orange” and “blue”.\n\nsample_percent\nSample percent of data for performing data tansformation.\n\nThe following script creates a data transformation report for the data.frame class object, heartfailure.\n\n\nheartfailure %>%\n  transformation_paged_report(target = \"death_event\", subtitle = \"heartfailure\",\n                              output_dir = \"./\", output_file = \"transformation.pdf\", \n                              theme = \"blue\")\n\n\n\nScreenshot of static report\nThe cover of the report is shown in the following figure.:\n\n\n\n(#fig:trans_paged_cover)The part of the report\n\n\n\nThe contents of the report is shown in the following figure.:\n\n\n\n(#fig:trans_paged_cntent)The dynamic contents of the report\n\n\n\n\n\n\n",
      "last_modified": "2021-11-28T14:59:34+09:00"
    }
  ],
  "collections": []
}
